{"cells": [{"cell_type": "code", "metadata": {"id": "0_0.3360300498988651"}, "execution_count": 1, "source": ["# Let's install Tensorflow 2.0:\n!!pip install -q tensorflow==2.0.0-rc0\n\n# And verify that it is now in its latest version:\nimport tensorflow as tf\nprint(tf.__version__)"], "outputs": [{"name": "stdout", "text": ["2.16.1\n"], "output_type": "stream"}]}, {"cell_type": "markdown", "metadata": {"id": "0_0.9132076026314686"}, "execution_count": 1, "source": ["## Importing the dataset"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "5654153313_0.8762837675980466"}, "execution_count": 2, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\ndf = nb.get_data('11561714547878799', '@SYS.USERID', 'True', {}, [])\n# The first function parameter refers to the service ID of the dataset.\n# @SYS.USERID refers to the user ID of the current user.\n# If the Sandbox key is 'false', it is referred to as a dataset, and if it's 'true', then the file is a sandbox file.\n# {} refers to the filters applied to the dataset.\n# [] refers to the data preparations applied to the dataset.\n# After [], users can specify the number of rows to limit the headcount of the dataset with a comma separator."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.07626144226203846"}, "execution_count": 3, "source": ["df.head()"], "outputs": [{"data": {"text/plain": ["             0       1       2       3       4       5       6       7       8       9      10      11      12      13      14      15      16      17      18      19      20      21      22      23      24      25      26      27      28      29      30      31      32      33      34      35      36      37      38      39      40      41      42      43      44      45      46      47      48      49      50      51      52      53      54      56      57      58      59 Target\n0.0442  0.0477  0.0049  0.0581  0.0278  0.0678  0.1664  0.1490  0.0974  0.1268  0.1109  0.2375  0.2007  0.2140  0.1109  0.2036  0.2468  0.6682  0.8345  0.8252  0.8017  0.8982  0.9664  0.8515  0.6626  0.3241  0.2054  0.5669  0.5726  0.4877  0.7532  0.7600  0.5185  0.4120  0.5560  0.5569  0.1336  0.3831  0.4611  0.4330  0.2556  0.1466  0.3489  0.2659  0.0944  0.1370  0.1344  0.0416  0.0719  0.0637  0.0210  0.0204  0.0216  0.0135  0.0055  0.0073  0.0080  0.0105  0.0059  0.0105      R\n0.0311  0.0491  0.0692  0.0831  0.0079  0.0200  0.0981  0.1016  0.2025  0.0767  0.1767  0.2555  0.2812  0.2722  0.3227  0.3463  0.5395  0.7911  0.9064  0.8701  0.7672  0.2957  0.4148  0.6043  0.3178  0.3482  0.6158  0.8049  0.6289  0.4999  0.5830  0.6660  0.4124  0.1260  0.2487  0.4676  0.5382  0.3150  0.2139  0.1848  0.1679  0.2328  0.1015  0.0713  0.0615  0.0779  0.0761  0.0845  0.0592  0.0068  0.0089  0.0087  0.0032  0.0130  0.0188  0.0101  0.0229  0.0182  0.0046  0.0038      R\n0.0206  0.0132  0.0533  0.0569  0.0647  0.1432  0.1344  0.2041  0.1571  0.1573  0.2327  0.1785  0.1507  0.1916  0.2061  0.2307  0.2360  0.1299  0.3812  0.5858  0.4497  0.4876  1.0000  0.8675  0.4718  0.5341  0.6197  0.7143  0.5605  0.3728  0.2481  0.1921  0.1386  0.3325  0.2883  0.3228  0.2607  0.2040  0.2396  0.1319  0.0683  0.0334  0.0716  0.0976  0.0787  0.0522  0.0500  0.0231  0.0221  0.0144  0.0307  0.0386  0.0147  0.0018  0.0100  0.0096  0.0077  0.0180  0.0109  0.0070      R\n0.0094  0.0166  0.0398  0.0359  0.0681  0.0706  0.1020  0.0893  0.0381  0.1328  0.1303  0.0273  0.0644  0.0712  0.1204  0.0717  0.1224  0.2349  0.3684  0.3918  0.4925  0.8793  0.9606  0.8786  0.6905  0.6937  0.5674  0.6540  0.7802  0.7575  0.5836  0.6316  0.8108  0.9039  0.8647  0.6695  0.4027  0.2370  0.2685  0.3662  0.3267  0.2200  0.2996  0.2205  0.1163  0.0635  0.0465  0.0422  0.0174  0.0172  0.0134  0.0141  0.0191  0.0145  0.0065  0.0129  0.0217  0.0087  0.0077  0.0122      R\n0.0333  0.0221  0.0270  0.0481  0.0679  0.0981  0.0843  0.1172  0.0759  0.0920  0.1475  0.0522  0.1119  0.0970  0.1174  0.1678  0.1642  0.1205  0.0494  0.1544  0.3485  0.6146  0.9146  0.9364  0.8677  0.8772  0.8553  0.8833  1.0000  0.8296  0.6601  0.5499  0.5716  0.6859  0.6825  0.5142  0.2750  0.1358  0.1551  0.2646  0.1994  0.1883  0.2746  0.1651  0.0575  0.0695  0.0598  0.0456  0.0021  0.0068  0.0036  0.0022  0.0032  0.0060  0.0054  0.0063  0.0143  0.0132  0.0051  0.0041      R"], "text/html": ["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>34</th>\n      <th>35</th>\n      <th>36</th>\n      <th>37</th>\n      <th>38</th>\n      <th>39</th>\n      <th>40</th>\n      <th>41</th>\n      <th>42</th>\n      <th>43</th>\n      <th>44</th>\n      <th>45</th>\n      <th>46</th>\n      <th>47</th>\n      <th>48</th>\n      <th>49</th>\n      <th>50</th>\n      <th>51</th>\n      <th>52</th>\n      <th>53</th>\n      <th>54</th>\n      <th>56</th>\n      <th>57</th>\n      <th>58</th>\n      <th>59</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0.0442</th>\n      <td>0.0477</td>\n      <td>0.0049</td>\n      <td>0.0581</td>\n      <td>0.0278</td>\n      <td>0.0678</td>\n      <td>0.1664</td>\n      <td>0.1490</td>\n      <td>0.0974</td>\n      <td>0.1268</td>\n      <td>0.1109</td>\n      <td>0.2375</td>\n      <td>0.2007</td>\n      <td>0.2140</td>\n      <td>0.1109</td>\n      <td>0.2036</td>\n      <td>0.2468</td>\n      <td>0.6682</td>\n      <td>0.8345</td>\n      <td>0.8252</td>\n      <td>0.8017</td>\n      <td>0.8982</td>\n      <td>0.9664</td>\n      <td>0.8515</td>\n      <td>0.6626</td>\n      <td>0.3241</td>\n      <td>0.2054</td>\n      <td>0.5669</td>\n      <td>0.5726</td>\n      <td>0.4877</td>\n      <td>0.7532</td>\n      <td>0.7600</td>\n      <td>0.5185</td>\n      <td>0.4120</td>\n      <td>0.5560</td>\n      <td>0.5569</td>\n      <td>0.1336</td>\n      <td>0.3831</td>\n      <td>0.4611</td>\n      <td>0.4330</td>\n      <td>0.2556</td>\n      <td>0.1466</td>\n      <td>0.3489</td>\n      <td>0.2659</td>\n      <td>0.0944</td>\n      <td>0.1370</td>\n      <td>0.1344</td>\n      <td>0.0416</td>\n      <td>0.0719</td>\n      <td>0.0637</td>\n      <td>0.0210</td>\n      <td>0.0204</td>\n      <td>0.0216</td>\n      <td>0.0135</td>\n      <td>0.0055</td>\n      <td>0.0073</td>\n      <td>0.0080</td>\n      <td>0.0105</td>\n      <td>0.0059</td>\n      <td>0.0105</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>0.0311</th>\n      <td>0.0491</td>\n      <td>0.0692</td>\n      <td>0.0831</td>\n      <td>0.0079</td>\n      <td>0.0200</td>\n      <td>0.0981</td>\n      <td>0.1016</td>\n      <td>0.2025</td>\n      <td>0.0767</td>\n      <td>0.1767</td>\n      <td>0.2555</td>\n      <td>0.2812</td>\n      <td>0.2722</td>\n      <td>0.3227</td>\n      <td>0.3463</td>\n      <td>0.5395</td>\n      <td>0.7911</td>\n      <td>0.9064</td>\n      <td>0.8701</td>\n      <td>0.7672</td>\n      <td>0.2957</td>\n      <td>0.4148</td>\n      <td>0.6043</td>\n      <td>0.3178</td>\n      <td>0.3482</td>\n      <td>0.6158</td>\n      <td>0.8049</td>\n      <td>0.6289</td>\n      <td>0.4999</td>\n      <td>0.5830</td>\n      <td>0.6660</td>\n      <td>0.4124</td>\n      <td>0.1260</td>\n      <td>0.2487</td>\n      <td>0.4676</td>\n      <td>0.5382</td>\n      <td>0.3150</td>\n      <td>0.2139</td>\n      <td>0.1848</td>\n      <td>0.1679</td>\n      <td>0.2328</td>\n      <td>0.1015</td>\n      <td>0.0713</td>\n      <td>0.0615</td>\n      <td>0.0779</td>\n      <td>0.0761</td>\n      <td>0.0845</td>\n      <td>0.0592</td>\n      <td>0.0068</td>\n      <td>0.0089</td>\n      <td>0.0087</td>\n      <td>0.0032</td>\n      <td>0.0130</td>\n      <td>0.0188</td>\n      <td>0.0101</td>\n      <td>0.0229</td>\n      <td>0.0182</td>\n      <td>0.0046</td>\n      <td>0.0038</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>0.0206</th>\n      <td>0.0132</td>\n      <td>0.0533</td>\n      <td>0.0569</td>\n      <td>0.0647</td>\n      <td>0.1432</td>\n      <td>0.1344</td>\n      <td>0.2041</td>\n      <td>0.1571</td>\n      <td>0.1573</td>\n      <td>0.2327</td>\n      <td>0.1785</td>\n      <td>0.1507</td>\n      <td>0.1916</td>\n      <td>0.2061</td>\n      <td>0.2307</td>\n      <td>0.2360</td>\n      <td>0.1299</td>\n      <td>0.3812</td>\n      <td>0.5858</td>\n      <td>0.4497</td>\n      <td>0.4876</td>\n      <td>1.0000</td>\n      <td>0.8675</td>\n      <td>0.4718</td>\n      <td>0.5341</td>\n      <td>0.6197</td>\n      <td>0.7143</td>\n      <td>0.5605</td>\n      <td>0.3728</td>\n      <td>0.2481</td>\n      <td>0.1921</td>\n      <td>0.1386</td>\n      <td>0.3325</td>\n      <td>0.2883</td>\n      <td>0.3228</td>\n      <td>0.2607</td>\n      <td>0.2040</td>\n      <td>0.2396</td>\n      <td>0.1319</td>\n      <td>0.0683</td>\n      <td>0.0334</td>\n      <td>0.0716</td>\n      <td>0.0976</td>\n      <td>0.0787</td>\n      <td>0.0522</td>\n      <td>0.0500</td>\n      <td>0.0231</td>\n      <td>0.0221</td>\n      <td>0.0144</td>\n      <td>0.0307</td>\n      <td>0.0386</td>\n      <td>0.0147</td>\n      <td>0.0018</td>\n      <td>0.0100</td>\n      <td>0.0096</td>\n      <td>0.0077</td>\n      <td>0.0180</td>\n      <td>0.0109</td>\n      <td>0.0070</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>0.0094</th>\n      <td>0.0166</td>\n      <td>0.0398</td>\n      <td>0.0359</td>\n      <td>0.0681</td>\n      <td>0.0706</td>\n      <td>0.1020</td>\n      <td>0.0893</td>\n      <td>0.0381</td>\n      <td>0.1328</td>\n      <td>0.1303</td>\n      <td>0.0273</td>\n      <td>0.0644</td>\n      <td>0.0712</td>\n      <td>0.1204</td>\n      <td>0.0717</td>\n      <td>0.1224</td>\n      <td>0.2349</td>\n      <td>0.3684</td>\n      <td>0.3918</td>\n      <td>0.4925</td>\n      <td>0.8793</td>\n      <td>0.9606</td>\n      <td>0.8786</td>\n      <td>0.6905</td>\n      <td>0.6937</td>\n      <td>0.5674</td>\n      <td>0.6540</td>\n      <td>0.7802</td>\n      <td>0.7575</td>\n      <td>0.5836</td>\n      <td>0.6316</td>\n      <td>0.8108</td>\n      <td>0.9039</td>\n      <td>0.8647</td>\n      <td>0.6695</td>\n      <td>0.4027</td>\n      <td>0.2370</td>\n      <td>0.2685</td>\n      <td>0.3662</td>\n      <td>0.3267</td>\n      <td>0.2200</td>\n      <td>0.2996</td>\n      <td>0.2205</td>\n      <td>0.1163</td>\n      <td>0.0635</td>\n      <td>0.0465</td>\n      <td>0.0422</td>\n      <td>0.0174</td>\n      <td>0.0172</td>\n      <td>0.0134</td>\n      <td>0.0141</td>\n      <td>0.0191</td>\n      <td>0.0145</td>\n      <td>0.0065</td>\n      <td>0.0129</td>\n      <td>0.0217</td>\n      <td>0.0087</td>\n      <td>0.0077</td>\n      <td>0.0122</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>0.0333</th>\n      <td>0.0221</td>\n      <td>0.0270</td>\n      <td>0.0481</td>\n      <td>0.0679</td>\n      <td>0.0981</td>\n      <td>0.0843</td>\n      <td>0.1172</td>\n      <td>0.0759</td>\n      <td>0.0920</td>\n      <td>0.1475</td>\n      <td>0.0522</td>\n      <td>0.1119</td>\n      <td>0.0970</td>\n      <td>0.1174</td>\n      <td>0.1678</td>\n      <td>0.1642</td>\n      <td>0.1205</td>\n      <td>0.0494</td>\n      <td>0.1544</td>\n      <td>0.3485</td>\n      <td>0.6146</td>\n      <td>0.9146</td>\n      <td>0.9364</td>\n      <td>0.8677</td>\n      <td>0.8772</td>\n      <td>0.8553</td>\n      <td>0.8833</td>\n      <td>1.0000</td>\n      <td>0.8296</td>\n      <td>0.6601</td>\n      <td>0.5499</td>\n      <td>0.5716</td>\n      <td>0.6859</td>\n      <td>0.6825</td>\n      <td>0.5142</td>\n      <td>0.2750</td>\n      <td>0.1358</td>\n      <td>0.1551</td>\n      <td>0.2646</td>\n      <td>0.1994</td>\n      <td>0.1883</td>\n      <td>0.2746</td>\n      <td>0.1651</td>\n      <td>0.0575</td>\n      <td>0.0695</td>\n      <td>0.0598</td>\n      <td>0.0456</td>\n      <td>0.0021</td>\n      <td>0.0068</td>\n      <td>0.0036</td>\n      <td>0.0022</td>\n      <td>0.0032</td>\n      <td>0.0060</td>\n      <td>0.0054</td>\n      <td>0.0063</td>\n      <td>0.0143</td>\n      <td>0.0132</td>\n      <td>0.0051</td>\n      <td>0.0041</td>\n      <td>R</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}, "metadata": {}, "execution_count": 4, "output_type": "execute_result"}]}, {"cell_type": "code", "metadata": {"id": "5654153313_0.3102097349924968"}, "execution_count": 4, "source": ["df.shape"], "outputs": [{"data": {"text/plain": "(175, 60)"}, "metadata": {}, "execution_count": 5, "output_type": "execute_result"}]}, {"cell_type": "code", "metadata": {"id": "5654153313_0.06669046438772153"}, "execution_count": 5, "source": ["df.describe()"], "outputs": [{"data": {"text/plain": ["                0           1           2           3           4           5           6           7           8           9          10          11          12          13          14          15          16          17          18          19          20          21          22          23          24          25          26          27          28          29          30          31          32          33          34          35          36          37          38          39          40          41          42          43          44          45          46          47          48          49          50          51          52          53          54          56          57          58          59\ncount  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000\nmean     0.039099    0.043781    0.054401    0.075051    0.104713    0.121982    0.136359    0.181121    0.209518    0.241015    0.251227    0.270678    0.298474    0.320623    0.372799    0.409113    0.447809    0.502383    0.561974    0.618351    0.637118    0.662147    0.692741    0.692774    0.717834    0.721132    0.709700    0.656487    0.593337    0.507393    0.436463    0.413743    0.398240    0.385444    0.370943    0.347531    0.327921    0.318503    0.305262    0.284508    0.280011    0.254199    0.219402    0.203865    0.163854    0.125604    0.094738    0.054166    0.020372    0.016258    0.013643    0.010694    0.010978    0.009119    0.008230    0.007662    0.008065    0.008019    0.006615\nstd      0.034592    0.039751    0.047694    0.055420    0.055374    0.059362    0.081155    0.115280    0.131929    0.129329    0.134464    0.136521    0.159073    0.201921    0.232076    0.263885    0.266479    0.263131    0.264012    0.257938    0.258085    0.254447    0.237271    0.245233    0.236203    0.240157    0.230954    0.235359    0.210756    0.209113    0.207692    0.199758    0.224776    0.261010    0.268594    0.238564    0.201827    0.188519    0.166054    0.163815    0.167152    0.142614    0.139410    0.159849    0.138279    0.087932    0.063697    0.035953    0.013500    0.012393    0.009908    0.007171    0.007545    0.007382    0.005774    0.005694    0.006680    0.006468    0.005334\nmin      0.000600    0.001500    0.005800    0.006700    0.011600    0.003300    0.005500    0.011700    0.011300    0.028900    0.023600    0.025200    0.027300    0.009200    0.042200    0.036700    0.037500    0.049400    0.074000    0.051200    0.021900    0.056300    0.023900    0.024000    0.092100    0.048100    0.028400    0.014400    0.082300    0.048200    0.077300    0.047700    0.021200    0.022300    0.008000    0.035100    0.038300    0.037100    0.011700    0.036000    0.005600    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000800    0.000500    0.001000    0.000600    0.000400    0.000300    0.000600    0.000100    0.000600\n25%      0.016550    0.018450    0.024350    0.039200    0.069000    0.084800    0.086700    0.105850    0.116300    0.144350    0.137950    0.165750    0.183100    0.164250    0.190500    0.203300    0.238300    0.298950    0.337500    0.411600    0.418050    0.473600    0.582150    0.574500    0.562800    0.566200    0.573500    0.471650    0.427950    0.349300    0.287850    0.257350    0.215300    0.170350    0.146150    0.157600    0.174250    0.176500    0.182950    0.160600    0.160250    0.155450    0.128150    0.090950    0.068400    0.066750    0.046250    0.028100    0.011500    0.008550    0.007350    0.004900    0.005350    0.003850    0.004400    0.003700    0.003600    0.003550    0.003000\n50%      0.029700    0.034200    0.044500    0.063000    0.093200    0.110800    0.113000    0.158300    0.184700    0.234000    0.255500    0.262400    0.282900    0.279400    0.295600    0.293400    0.342400    0.432700    0.549200    0.652700    0.698800    0.724600    0.736000    0.751900    0.782500    0.765200    0.736000    0.703200    0.615100    0.490100    0.424100    0.393900    0.376300    0.309300    0.289700    0.269900    0.287100    0.278900    0.277100    0.257500    0.254600    0.238000    0.177900    0.149900    0.118100    0.103400    0.084500    0.047500    0.017900    0.014000    0.011700    0.009300    0.009300    0.007200    0.006800    0.005800    0.006000    0.006300    0.005300\n75%      0.047300    0.055050    0.065700    0.099400    0.134150    0.153050    0.169800    0.236300    0.267800    0.306700    0.328600    0.335800    0.390000    0.452650    0.537950    0.640050    0.676600    0.750650    0.809750    0.826850    0.841900    0.866550    0.878700    0.889800    0.909550    0.925650    0.908850    0.856050    0.731350    0.643200    0.555250    0.539700    0.565350    0.591400    0.541500    0.481350    0.418500    0.427650    0.416950    0.389150    0.387800    0.341050    0.279500    0.239300    0.216050    0.163750    0.126900    0.073000    0.025450    0.021150    0.016750    0.015000    0.014450    0.012200    0.010350    0.010200    0.010500    0.010350    0.008700\nmax      0.233900    0.305900    0.426400    0.401000    0.382300    0.372900    0.459000    0.682800    0.710600    0.734200    0.577100    0.713100    0.997000    0.913700    0.975100    1.000000    1.000000    1.000000    1.000000    1.000000    1.000000    1.000000    1.000000    1.000000    1.000000    1.000000    1.000000    1.000000    1.000000    0.965700    0.930600    1.000000    0.953600    1.000000    1.000000    0.949700    0.948000    0.884900    0.897900    0.775100    0.824600    0.773300    0.776200    0.703400    0.729200    0.552200    0.333900    0.179400    0.082500    0.100400    0.070900    0.039000    0.035200    0.044700    0.039400    0.035500    0.044000    0.036400    0.043900"], "text/html": ["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>34</th>\n      <th>35</th>\n      <th>36</th>\n      <th>37</th>\n      <th>38</th>\n      <th>39</th>\n      <th>40</th>\n      <th>41</th>\n      <th>42</th>\n      <th>43</th>\n      <th>44</th>\n      <th>45</th>\n      <th>46</th>\n      <th>47</th>\n      <th>48</th>\n      <th>49</th>\n      <th>50</th>\n      <th>51</th>\n      <th>52</th>\n      <th>53</th>\n      <th>54</th>\n      <th>56</th>\n      <th>57</th>\n      <th>58</th>\n      <th>59</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.039099</td>\n      <td>0.043781</td>\n      <td>0.054401</td>\n      <td>0.075051</td>\n      <td>0.104713</td>\n      <td>0.121982</td>\n      <td>0.136359</td>\n      <td>0.181121</td>\n      <td>0.209518</td>\n      <td>0.241015</td>\n      <td>0.251227</td>\n      <td>0.270678</td>\n      <td>0.298474</td>\n      <td>0.320623</td>\n      <td>0.372799</td>\n      <td>0.409113</td>\n      <td>0.447809</td>\n      <td>0.502383</td>\n      <td>0.561974</td>\n      <td>0.618351</td>\n      <td>0.637118</td>\n      <td>0.662147</td>\n      <td>0.692741</td>\n      <td>0.692774</td>\n      <td>0.717834</td>\n      <td>0.721132</td>\n      <td>0.709700</td>\n      <td>0.656487</td>\n      <td>0.593337</td>\n      <td>0.507393</td>\n      <td>0.436463</td>\n      <td>0.413743</td>\n      <td>0.398240</td>\n      <td>0.385444</td>\n      <td>0.370943</td>\n      <td>0.347531</td>\n      <td>0.327921</td>\n      <td>0.318503</td>\n      <td>0.305262</td>\n      <td>0.284508</td>\n      <td>0.280011</td>\n      <td>0.254199</td>\n      <td>0.219402</td>\n      <td>0.203865</td>\n      <td>0.163854</td>\n      <td>0.125604</td>\n      <td>0.094738</td>\n      <td>0.054166</td>\n      <td>0.020372</td>\n      <td>0.016258</td>\n      <td>0.013643</td>\n      <td>0.010694</td>\n      <td>0.010978</td>\n      <td>0.009119</td>\n      <td>0.008230</td>\n      <td>0.007662</td>\n      <td>0.008065</td>\n      <td>0.008019</td>\n      <td>0.006615</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.034592</td>\n      <td>0.039751</td>\n      <td>0.047694</td>\n      <td>0.055420</td>\n      <td>0.055374</td>\n      <td>0.059362</td>\n      <td>0.081155</td>\n      <td>0.115280</td>\n      <td>0.131929</td>\n      <td>0.129329</td>\n      <td>0.134464</td>\n      <td>0.136521</td>\n      <td>0.159073</td>\n      <td>0.201921</td>\n      <td>0.232076</td>\n      <td>0.263885</td>\n      <td>0.266479</td>\n      <td>0.263131</td>\n      <td>0.264012</td>\n      <td>0.257938</td>\n      <td>0.258085</td>\n      <td>0.254447</td>\n      <td>0.237271</td>\n      <td>0.245233</td>\n      <td>0.236203</td>\n      <td>0.240157</td>\n      <td>0.230954</td>\n      <td>0.235359</td>\n      <td>0.210756</td>\n      <td>0.209113</td>\n      <td>0.207692</td>\n      <td>0.199758</td>\n      <td>0.224776</td>\n      <td>0.261010</td>\n      <td>0.268594</td>\n      <td>0.238564</td>\n      <td>0.201827</td>\n      <td>0.188519</td>\n      <td>0.166054</td>\n      <td>0.163815</td>\n      <td>0.167152</td>\n      <td>0.142614</td>\n      <td>0.139410</td>\n      <td>0.159849</td>\n      <td>0.138279</td>\n      <td>0.087932</td>\n      <td>0.063697</td>\n      <td>0.035953</td>\n      <td>0.013500</td>\n      <td>0.012393</td>\n      <td>0.009908</td>\n      <td>0.007171</td>\n      <td>0.007545</td>\n      <td>0.007382</td>\n      <td>0.005774</td>\n      <td>0.005694</td>\n      <td>0.006680</td>\n      <td>0.006468</td>\n      <td>0.005334</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000600</td>\n      <td>0.001500</td>\n      <td>0.005800</td>\n      <td>0.006700</td>\n      <td>0.011600</td>\n      <td>0.003300</td>\n      <td>0.005500</td>\n      <td>0.011700</td>\n      <td>0.011300</td>\n      <td>0.028900</td>\n      <td>0.023600</td>\n      <td>0.025200</td>\n      <td>0.027300</td>\n      <td>0.009200</td>\n      <td>0.042200</td>\n      <td>0.036700</td>\n      <td>0.037500</td>\n      <td>0.049400</td>\n      <td>0.074000</td>\n      <td>0.051200</td>\n      <td>0.021900</td>\n      <td>0.056300</td>\n      <td>0.023900</td>\n      <td>0.024000</td>\n      <td>0.092100</td>\n      <td>0.048100</td>\n      <td>0.028400</td>\n      <td>0.014400</td>\n      <td>0.082300</td>\n      <td>0.048200</td>\n      <td>0.077300</td>\n      <td>0.047700</td>\n      <td>0.021200</td>\n      <td>0.022300</td>\n      <td>0.008000</td>\n      <td>0.035100</td>\n      <td>0.038300</td>\n      <td>0.037100</td>\n      <td>0.011700</td>\n      <td>0.036000</td>\n      <td>0.005600</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000800</td>\n      <td>0.000500</td>\n      <td>0.001000</td>\n      <td>0.000600</td>\n      <td>0.000400</td>\n      <td>0.000300</td>\n      <td>0.000600</td>\n      <td>0.000100</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.016550</td>\n      <td>0.018450</td>\n      <td>0.024350</td>\n      <td>0.039200</td>\n      <td>0.069000</td>\n      <td>0.084800</td>\n      <td>0.086700</td>\n      <td>0.105850</td>\n      <td>0.116300</td>\n      <td>0.144350</td>\n      <td>0.137950</td>\n      <td>0.165750</td>\n      <td>0.183100</td>\n      <td>0.164250</td>\n      <td>0.190500</td>\n      <td>0.203300</td>\n      <td>0.238300</td>\n      <td>0.298950</td>\n      <td>0.337500</td>\n      <td>0.411600</td>\n      <td>0.418050</td>\n      <td>0.473600</td>\n      <td>0.582150</td>\n      <td>0.574500</td>\n      <td>0.562800</td>\n      <td>0.566200</td>\n      <td>0.573500</td>\n      <td>0.471650</td>\n      <td>0.427950</td>\n      <td>0.349300</td>\n      <td>0.287850</td>\n      <td>0.257350</td>\n      <td>0.215300</td>\n      <td>0.170350</td>\n      <td>0.146150</td>\n      <td>0.157600</td>\n      <td>0.174250</td>\n      <td>0.176500</td>\n      <td>0.182950</td>\n      <td>0.160600</td>\n      <td>0.160250</td>\n      <td>0.155450</td>\n      <td>0.128150</td>\n      <td>0.090950</td>\n      <td>0.068400</td>\n      <td>0.066750</td>\n      <td>0.046250</td>\n      <td>0.028100</td>\n      <td>0.011500</td>\n      <td>0.008550</td>\n      <td>0.007350</td>\n      <td>0.004900</td>\n      <td>0.005350</td>\n      <td>0.003850</td>\n      <td>0.004400</td>\n      <td>0.003700</td>\n      <td>0.003600</td>\n      <td>0.003550</td>\n      <td>0.003000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.029700</td>\n      <td>0.034200</td>\n      <td>0.044500</td>\n      <td>0.063000</td>\n      <td>0.093200</td>\n      <td>0.110800</td>\n      <td>0.113000</td>\n      <td>0.158300</td>\n      <td>0.184700</td>\n      <td>0.234000</td>\n      <td>0.255500</td>\n      <td>0.262400</td>\n      <td>0.282900</td>\n      <td>0.279400</td>\n      <td>0.295600</td>\n      <td>0.293400</td>\n      <td>0.342400</td>\n      <td>0.432700</td>\n      <td>0.549200</td>\n      <td>0.652700</td>\n      <td>0.698800</td>\n      <td>0.724600</td>\n      <td>0.736000</td>\n      <td>0.751900</td>\n      <td>0.782500</td>\n      <td>0.765200</td>\n      <td>0.736000</td>\n      <td>0.703200</td>\n      <td>0.615100</td>\n      <td>0.490100</td>\n      <td>0.424100</td>\n      <td>0.393900</td>\n      <td>0.376300</td>\n      <td>0.309300</td>\n      <td>0.289700</td>\n      <td>0.269900</td>\n      <td>0.287100</td>\n      <td>0.278900</td>\n      <td>0.277100</td>\n      <td>0.257500</td>\n      <td>0.254600</td>\n      <td>0.238000</td>\n      <td>0.177900</td>\n      <td>0.149900</td>\n      <td>0.118100</td>\n      <td>0.103400</td>\n      <td>0.084500</td>\n      <td>0.047500</td>\n      <td>0.017900</td>\n      <td>0.014000</td>\n      <td>0.011700</td>\n      <td>0.009300</td>\n      <td>0.009300</td>\n      <td>0.007200</td>\n      <td>0.006800</td>\n      <td>0.005800</td>\n      <td>0.006000</td>\n      <td>0.006300</td>\n      <td>0.005300</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.047300</td>\n      <td>0.055050</td>\n      <td>0.065700</td>\n      <td>0.099400</td>\n      <td>0.134150</td>\n      <td>0.153050</td>\n      <td>0.169800</td>\n      <td>0.236300</td>\n      <td>0.267800</td>\n      <td>0.306700</td>\n      <td>0.328600</td>\n      <td>0.335800</td>\n      <td>0.390000</td>\n      <td>0.452650</td>\n      <td>0.537950</td>\n      <td>0.640050</td>\n      <td>0.676600</td>\n      <td>0.750650</td>\n      <td>0.809750</td>\n      <td>0.826850</td>\n      <td>0.841900</td>\n      <td>0.866550</td>\n      <td>0.878700</td>\n      <td>0.889800</td>\n      <td>0.909550</td>\n      <td>0.925650</td>\n      <td>0.908850</td>\n      <td>0.856050</td>\n      <td>0.731350</td>\n      <td>0.643200</td>\n      <td>0.555250</td>\n      <td>0.539700</td>\n      <td>0.565350</td>\n      <td>0.591400</td>\n      <td>0.541500</td>\n      <td>0.481350</td>\n      <td>0.418500</td>\n      <td>0.427650</td>\n      <td>0.416950</td>\n      <td>0.389150</td>\n      <td>0.387800</td>\n      <td>0.341050</td>\n      <td>0.279500</td>\n      <td>0.239300</td>\n      <td>0.216050</td>\n      <td>0.163750</td>\n      <td>0.126900</td>\n      <td>0.073000</td>\n      <td>0.025450</td>\n      <td>0.021150</td>\n      <td>0.016750</td>\n      <td>0.015000</td>\n      <td>0.014450</td>\n      <td>0.012200</td>\n      <td>0.010350</td>\n      <td>0.010200</td>\n      <td>0.010500</td>\n      <td>0.010350</td>\n      <td>0.008700</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.233900</td>\n      <td>0.305900</td>\n      <td>0.426400</td>\n      <td>0.401000</td>\n      <td>0.382300</td>\n      <td>0.372900</td>\n      <td>0.459000</td>\n      <td>0.682800</td>\n      <td>0.710600</td>\n      <td>0.734200</td>\n      <td>0.577100</td>\n      <td>0.713100</td>\n      <td>0.997000</td>\n      <td>0.913700</td>\n      <td>0.975100</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.965700</td>\n      <td>0.930600</td>\n      <td>1.000000</td>\n      <td>0.953600</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.949700</td>\n      <td>0.948000</td>\n      <td>0.884900</td>\n      <td>0.897900</td>\n      <td>0.775100</td>\n      <td>0.824600</td>\n      <td>0.773300</td>\n      <td>0.776200</td>\n      <td>0.703400</td>\n      <td>0.729200</td>\n      <td>0.552200</td>\n      <td>0.333900</td>\n      <td>0.179400</td>\n      <td>0.082500</td>\n      <td>0.100400</td>\n      <td>0.070900</td>\n      <td>0.039000</td>\n      <td>0.035200</td>\n      <td>0.044700</td>\n      <td>0.039400</td>\n      <td>0.035500</td>\n      <td>0.044000</td>\n      <td>0.036400</td>\n      <td>0.043900</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}, "metadata": {}, "execution_count": 6, "output_type": "execute_result"}]}, {"cell_type": "code", "metadata": {"id": "5654153313_0.8609577777460835"}, "execution_count": 6, "source": ["df['Target'].value_counts()"], "outputs": [{"data": {"text/plain": "Target\nM    111\nR     64\nName: count, dtype: int64"}, "metadata": {}, "execution_count": 7, "output_type": "execute_result"}]}, {"cell_type": "code", "metadata": {"id": "5654153313_0.45640754632469216"}, "execution_count": 7, "source": ["# Count Plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.countplot(x='Target', data=df)\nplt.title('Distribution of Classes')\nplt.show()"], "outputs": [{"data": {"text/plain": ["<Figure size 640x480 with 1 Axes>"], "image/png": "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"}, "metadata": {}, "output_type": "display_data"}]}, {"cell_type": "markdown", "metadata": {"id": "0_0.8086314137893598"}, "execution_count": 1, "source": ["## Let's talk about the model...\n\nWe will be using a very simple model, a feed-forward multi-layer perceptron.\n\n### Let's create the model with Keras!\n\nFirst of all, let's import what we'll use:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "5654153313_0.43153947672353477"}, "execution_count": 8, "source": ["import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\nX = df.drop(columns=['Target'], axis=1)\ny = df['Target']\n\n# Encode target labels\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define preprocessing steps\nnumeric_features = X_train.select_dtypes(include=['float64']).columns\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features)\n    ])\n\n# Define the MLPClassifier model\nmlp_model = MLPClassifier(hidden_layer_sizes=(64, 64, 32, 32), activation='relu', solver='adam', max_iter=1000)\n\n# Create a pipeline\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                           ('classifier', mlp_model)])\n\n# Train the model\npipeline.fit(X_train, y_train)\n"], "outputs": [{"data": {"text/plain": ["Pipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n       '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24',\n       '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36',\n       '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48',\n       '49', '50', '51', '52', '53', '54', '56', '57', '58', '59'],\n      dtype='object'))])),\n                ('classifier',\n                 MLPClassifier(hidden_layer_sizes=(64, 64, 32, 32),\n                               max_iter=1000))])"], "text/html": ["<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\u25b8\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\u25be\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n                 ColumnTransformer(transformers=[(&#x27;num&#x27;,\n                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n                                                                   SimpleImputer(strategy=&#x27;median&#x27;)),\n                                                                  (&#x27;scaler&#x27;,\n                                                                   StandardScaler())]),\n                                                  Index([&#x27;0&#x27;, &#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;, &#x27;4&#x27;, &#x27;5&#x27;, &#x27;6&#x27;, &#x27;7&#x27;, &#x27;8&#x27;, &#x27;9&#x27;, &#x27;10&#x27;, &#x27;11&#x27;, &#x27;12&#x27;,\n       &#x27;13&#x27;, &#x27;14&#x27;, &#x27;15&#x27;, &#x27;16&#x27;, &#x27;17&#x27;, &#x27;18&#x27;, &#x27;19&#x27;, &#x27;20&#x27;, &#x27;21&#x27;, &#x27;22&#x27;, &#x27;23&#x27;, &#x27;24&#x27;,\n       &#x27;25&#x27;, &#x27;26&#x27;, &#x27;27&#x27;, &#x27;28&#x27;, &#x27;29&#x27;, &#x27;30&#x27;, &#x27;31&#x27;, &#x27;32&#x27;, &#x27;33&#x27;, &#x27;34&#x27;, &#x27;35&#x27;, &#x27;36&#x27;,\n       &#x27;37&#x27;, &#x27;38&#x27;, &#x27;39&#x27;, &#x27;40&#x27;, &#x27;41&#x27;, &#x27;42&#x27;, &#x27;43&#x27;, &#x27;44&#x27;, &#x27;45&#x27;, &#x27;46&#x27;, &#x27;47&#x27;, &#x27;48&#x27;,\n       &#x27;49&#x27;, &#x27;50&#x27;, &#x27;51&#x27;, &#x27;52&#x27;, &#x27;53&#x27;, &#x27;54&#x27;, &#x27;56&#x27;, &#x27;57&#x27;, &#x27;58&#x27;, &#x27;59&#x27;],\n      dtype=&#x27;object&#x27;))])),\n                (&#x27;classifier&#x27;,\n                 MLPClassifier(hidden_layer_sizes=(64, 64, 32, 32),\n                               max_iter=1000))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n                 ColumnTransformer(transformers=[(&#x27;num&#x27;,\n                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n                                                                   SimpleImputer(strategy=&#x27;median&#x27;)),\n                                                                  (&#x27;scaler&#x27;,\n                                                                   StandardScaler())]),\n                                                  Index([&#x27;0&#x27;, &#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;, &#x27;4&#x27;, &#x27;5&#x27;, &#x27;6&#x27;, &#x27;7&#x27;, &#x27;8&#x27;, &#x27;9&#x27;, &#x27;10&#x27;, &#x27;11&#x27;, &#x27;12&#x27;,\n       &#x27;13&#x27;, &#x27;14&#x27;, &#x27;15&#x27;, &#x27;16&#x27;, &#x27;17&#x27;, &#x27;18&#x27;, &#x27;19&#x27;, &#x27;20&#x27;, &#x27;21&#x27;, &#x27;22&#x27;, &#x27;23&#x27;, &#x27;24&#x27;,\n       &#x27;25&#x27;, &#x27;26&#x27;, &#x27;27&#x27;, &#x27;28&#x27;, &#x27;29&#x27;, &#x27;30&#x27;, &#x27;31&#x27;, &#x27;32&#x27;, &#x27;33&#x27;, &#x27;34&#x27;, &#x27;35&#x27;, &#x27;36&#x27;,\n       &#x27;37&#x27;, &#x27;38&#x27;, &#x27;39&#x27;, &#x27;40&#x27;, &#x27;41&#x27;, &#x27;42&#x27;, &#x27;43&#x27;, &#x27;44&#x27;, &#x27;45&#x27;, &#x27;46&#x27;, &#x27;47&#x27;, &#x27;48&#x27;,\n       &#x27;49&#x27;, &#x27;50&#x27;, &#x27;51&#x27;, &#x27;52&#x27;, &#x27;53&#x27;, &#x27;54&#x27;, &#x27;56&#x27;, &#x27;57&#x27;, &#x27;58&#x27;, &#x27;59&#x27;],\n      dtype=&#x27;object&#x27;))])),\n                (&#x27;classifier&#x27;,\n                 MLPClassifier(hidden_layer_sizes=(64, 64, 32, 32),\n                               max_iter=1000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;num&#x27;,\n                                 Pipeline(steps=[(&#x27;imputer&#x27;,\n                                                  SimpleImputer(strategy=&#x27;median&#x27;)),\n                                                 (&#x27;scaler&#x27;, StandardScaler())]),\n                                 Index([&#x27;0&#x27;, &#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;, &#x27;4&#x27;, &#x27;5&#x27;, &#x27;6&#x27;, &#x27;7&#x27;, &#x27;8&#x27;, &#x27;9&#x27;, &#x27;10&#x27;, &#x27;11&#x27;, &#x27;12&#x27;,\n       &#x27;13&#x27;, &#x27;14&#x27;, &#x27;15&#x27;, &#x27;16&#x27;, &#x27;17&#x27;, &#x27;18&#x27;, &#x27;19&#x27;, &#x27;20&#x27;, &#x27;21&#x27;, &#x27;22&#x27;, &#x27;23&#x27;, &#x27;24&#x27;,\n       &#x27;25&#x27;, &#x27;26&#x27;, &#x27;27&#x27;, &#x27;28&#x27;, &#x27;29&#x27;, &#x27;30&#x27;, &#x27;31&#x27;, &#x27;32&#x27;, &#x27;33&#x27;, &#x27;34&#x27;, &#x27;35&#x27;, &#x27;36&#x27;,\n       &#x27;37&#x27;, &#x27;38&#x27;, &#x27;39&#x27;, &#x27;40&#x27;, &#x27;41&#x27;, &#x27;42&#x27;, &#x27;43&#x27;, &#x27;44&#x27;, &#x27;45&#x27;, &#x27;46&#x27;, &#x27;47&#x27;, &#x27;48&#x27;,\n       &#x27;49&#x27;, &#x27;50&#x27;, &#x27;51&#x27;, &#x27;52&#x27;, &#x27;53&#x27;, &#x27;54&#x27;, &#x27;56&#x27;, &#x27;57&#x27;, &#x27;58&#x27;, &#x27;59&#x27;],\n      dtype=&#x27;object&#x27;))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">num</label><div class=\"sk-toggleable__content\"><pre>Index([&#x27;0&#x27;, &#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;, &#x27;4&#x27;, &#x27;5&#x27;, &#x27;6&#x27;, &#x27;7&#x27;, &#x27;8&#x27;, &#x27;9&#x27;, &#x27;10&#x27;, &#x27;11&#x27;, &#x27;12&#x27;,\n       &#x27;13&#x27;, &#x27;14&#x27;, &#x27;15&#x27;, &#x27;16&#x27;, &#x27;17&#x27;, &#x27;18&#x27;, &#x27;19&#x27;, &#x27;20&#x27;, &#x27;21&#x27;, &#x27;22&#x27;, &#x27;23&#x27;, &#x27;24&#x27;,\n       &#x27;25&#x27;, &#x27;26&#x27;, &#x27;27&#x27;, &#x27;28&#x27;, &#x27;29&#x27;, &#x27;30&#x27;, &#x27;31&#x27;, &#x27;32&#x27;, &#x27;33&#x27;, &#x27;34&#x27;, &#x27;35&#x27;, &#x27;36&#x27;,\n       &#x27;37&#x27;, &#x27;38&#x27;, &#x27;39&#x27;, &#x27;40&#x27;, &#x27;41&#x27;, &#x27;42&#x27;, &#x27;43&#x27;, &#x27;44&#x27;, &#x27;45&#x27;, &#x27;46&#x27;, &#x27;47&#x27;, &#x27;48&#x27;,\n       &#x27;49&#x27;, &#x27;50&#x27;, &#x27;51&#x27;, &#x27;52&#x27;, &#x27;53&#x27;, &#x27;54&#x27;, &#x27;56&#x27;, &#x27;57&#x27;, &#x27;58&#x27;, &#x27;59&#x27;],\n      dtype=&#x27;object&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(64, 64, 32, 32), max_iter=1000)</pre></div></div></div></div></div></div></div>"]}, "metadata": {}, "execution_count": 9, "output_type": "execute_result"}]}, {"cell_type": "code", "metadata": {"id": "5654153313_0.9953838799035728"}, "execution_count": 9, "source": ["# Make predictions\ny_pred = pipeline.predict(X_test)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)"], "outputs": [{"name": "stdout", "text": ["Accuracy: 0.8571428571428571\n"], "output_type": "stream"}]}, {"cell_type": "code", "metadata": {"id": "5654153313_0.8058965169260286"}, "execution_count": 11, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\nsaved_model = nb.save_model(model = pipeline, modelName = 'TF_CLF', modelType = 'ml', X = None, y = None, estimator_type='')\n#X and y are training datasets to get explainer dashboard.\n#estimator_type is to specify algorithm type i.e., classification and regression.\n#Only 'ml\u2019 models with tabular data as input will support in Explainer Dashboard.\n#Choose modelType = 'ml' for machine learning models, modelType = 'cv' for computer vision models and modelType = 'dp' for data transformation pickle files. \n#Provide \u2018column_headers\u2019 as a parameter if they have to be saved in the model.\n#If using custom layer in keras, use native save functionality from keras."], "outputs": [{"name": "stdout", "text": ["WARN: Training data is not provided. Unable to generate Explainer Dashboard\n"], "output_type": "stream"}]}, {"cell_type": "code", "metadata": {"id": "5654153313_0.44984530157703273"}, "execution_count": null, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\nloaded_model = nb.load_saved_model('11561714557683756')"], "outputs": []}], "metadata": {}, "nbformat": 4, "nbformat_minor": 2}