{"cells": [{"cell_type": "markdown", "metadata": {"id": "0_0.2460738206881916"}, "execution_count": null, "source": ["<h2>Building a Sentiment analysis model for Movie reviews using LSTM</h2>\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.41851759978485537"}, "execution_count": null, "source": ["#### Load data"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.22798224764455077"}, "execution_count": null, "source": ["- Data can be downloaded from [Kaggle](https://www.kaggle.com/c/word2vec-nlp-tutorial/data).\n- The dataset contains 25000 movie reviews with their sentiment value (1 -> positive sentiment, 0 -> negative sentiment).\n- We will use the word2vec model build in previous exercise and use it for building a model for sentiment analysis.\n- Download 'labeledTrainData.tsv.zip' from Kaggle for this exercise."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.4252945871502525"}, "execution_count": null, "source": ["import pandas as pd"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.3465013569392217"}, "execution_count": null, "source": ["from google.colab import drive\ndrive.mount('/content/gdrive')"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.7235643330201504"}, "execution_count": null, "source": ["#Load dataset in memory.\ndf = pd.read_csv('/content/gdrive/MyDrive/labeledTrainData.tsv',  header=0, delimiter=\"\\t\", quoting=3)\n\n#Check number of records and columns\nprint(df.shape)"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.283719141586422"}, "execution_count": null, "source": ["#Preview some records\ndf.head()"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.03199517432039567"}, "execution_count": null, "source": ["#### Data Preprocessing"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.9549260981860748"}, "execution_count": null, "source": [" **Split Data** into Training and Test Data"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.7539682673572299"}, "execution_count": null, "source": ["from sklearn.model_selection import train_test_split"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.00251724842925638"}, "execution_count": null, "source": ["#We will use 80% of examples for training and 20% for test\nX_train, X_test, y_train, y_test = train_test_split(df['review'],\n                                                    df['sentiment'],\n                                                    test_size=0.2,\n                                                    random_state=42)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.5350827320283309"}, "execution_count": null, "source": ["**Build Tokenizer** to get Number sequences for each review"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.34193943433414575"}, "execution_count": null, "source": ["import tensorflow as tf"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.3371962947429208"}, "execution_count": null, "source": ["#Vocab size - we will limit vocabulary to 10000\ntop_words = 10000\n\n#Build tokenizer\nt = tf.keras.preprocessing.text.Tokenizer(num_words=top_words)\nt.fit_on_texts(X_train.tolist())"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.046106223989612616"}, "execution_count": null, "source": ["#Get the word index for each of the word in the review\nX_train = t.texts_to_sequences(X_train.tolist())\nX_test = t.texts_to_sequences(X_test.tolist())"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.8730440749206905"}, "execution_count": null, "source": ["#Check out first training review\nX_train[0]"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.8467049656675085"}, "execution_count": null, "source": ["**Pad sequences** to make each review size equalGet the word index for each of the word in the review"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.6791516143759653"}, "execution_count": null, "source": ["#Check length of 101st example and 201st example\nlen(X_train[100]), len(X_train[200])"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.7917368112579237"}, "execution_count": null, "source": ["#Each review size\nmax_review_length = 300\n\nX_train = tf.keras.preprocessing.sequence.pad_sequences(X_train,maxlen=max_review_length,padding='post')\nX_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_review_length, padding='post')"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.014129328043817724"}, "execution_count": null, "source": ["#Check length of 101st example and 201st example again\nlen(X_train[100]), len(X_train[200])"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.9212562008282206"}, "execution_count": null, "source": ["#### Build the Graph"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.9674331098273019"}, "execution_count": null, "source": ["#Start a Sequential Model\nmodel = tf.keras.Sequential()"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.939465037077821"}, "execution_count": null, "source": ["embedding_vector_length=50"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.05966914092832343"}, "execution_count": null, "source": ["Add **Embedding layer**\n\nHere we are training Word2Vec model as part of sentiment analysis. We are not providing pre-trained weights unlike the last exercise. Also this layer is a 'trainable' layer and will build Word2Vec embeddings for each word in vocabulary."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.5305749737108332"}, "execution_count": null, "source": ["model.add(tf.keras.layers.Embedding(top_words + 1, #Indexes that we need to deal with\n                                    embedding_vector_length, #embedding_size i.e 50 in this case\n                                    input_length=max_review_length, #Size of each review i.e 300 in this case\n                                ))"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.8686243699846026"}, "execution_count": null, "source": ["#Check output of model size\nmodel.output"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.26499153410936027"}, "execution_count": null, "source": ["Output from Embedding is 3 dimension :\n\n- batch_size x max_review_length (300) x embedding_vector_length (50)."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.6095033891041501"}, "execution_count": null, "source": ["Let's add LSTM as hidden layer"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.10289773602920915"}, "execution_count": null, "source": ["model.add(tf.keras.layers.LSTM(128)) #128 is size of hidden state and cell state\nmodel.add(tf.keras.layers.Dropout(0.25))"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.9734341499521573"}, "execution_count": null, "source": ["Add output layer"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.02177821423955484"}, "execution_count": null, "source": ["#We need one output - probability of positive sentiment\nmodel.add(tf.keras.layers.Dense(1,activation='sigmoid'))"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.6977451211981387"}, "execution_count": null, "source": ["Compile the model"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.031033002301580792"}, "execution_count": null, "source": ["model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.20582598535101426"}, "execution_count": null, "source": ["#Check model\nmodel.summary()"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.9261711679160984"}, "execution_count": null, "source": ["#### Execute the graph"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.19036709194739032"}, "execution_count": null, "source": ["model.fit(X_train,y_train,\n          epochs=10,\n          batch_size=128,\n          validation_data=(X_test, y_test))"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.33852011479626487"}, "execution_count": null, "source": ["Predicting from train model"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.5738073536042392"}, "execution_count": null, "source": ["#feeding 101st test example\nmodel.predict(X_test[100:101])"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.26759606637102684"}, "execution_count": null, "source": ["Try changing the size of hidden state /Cell state in LSTM to improve the model."], "outputs": []}], "metadata": {}, "nbformat": 4, "nbformat_minor": 2}