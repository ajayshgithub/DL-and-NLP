{"cells": [{"cell_type": "code", "metadata": {"id": "5870217729_0.26333639090987115"}, "execution_count": 0, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\ndf = nb.get_data('11561715594123109', '@SYS.USERID', 'True', {}, [])\ndf"], "outputs": [{"data": {"text/plain": ["             0       1       2       3       4       5       6       7       8       9      10      11      12      13      14      15      16      17      18      19      20      21      22      23      24      25      26      27      28      29      30      31      32      33      34      35      36      37      38      39      40      41      42      43      44      45      46      47      48      49      50      51      52      53      54      56      57      58      59 60\n0.0442  0.0477  0.0049  0.0581  0.0278  0.0678  0.1664  0.1490  0.0974  0.1268  0.1109  0.2375  0.2007  0.2140  0.1109  0.2036  0.2468  0.6682  0.8345  0.8252  0.8017  0.8982  0.9664  0.8515  0.6626  0.3241  0.2054  0.5669  0.5726  0.4877  0.7532  0.7600  0.5185  0.4120  0.5560  0.5569  0.1336  0.3831  0.4611  0.4330  0.2556  0.1466  0.3489  0.2659  0.0944  0.1370  0.1344  0.0416  0.0719  0.0637  0.0210  0.0204  0.0216  0.0135  0.0055  0.0073  0.0080  0.0105  0.0059  0.0105  R\n0.0311  0.0491  0.0692  0.0831  0.0079  0.0200  0.0981  0.1016  0.2025  0.0767  0.1767  0.2555  0.2812  0.2722  0.3227  0.3463  0.5395  0.7911  0.9064  0.8701  0.7672  0.2957  0.4148  0.6043  0.3178  0.3482  0.6158  0.8049  0.6289  0.4999  0.5830  0.6660  0.4124  0.1260  0.2487  0.4676  0.5382  0.3150  0.2139  0.1848  0.1679  0.2328  0.1015  0.0713  0.0615  0.0779  0.0761  0.0845  0.0592  0.0068  0.0089  0.0087  0.0032  0.0130  0.0188  0.0101  0.0229  0.0182  0.0046  0.0038  R\n0.0206  0.0132  0.0533  0.0569  0.0647  0.1432  0.1344  0.2041  0.1571  0.1573  0.2327  0.1785  0.1507  0.1916  0.2061  0.2307  0.2360  0.1299  0.3812  0.5858  0.4497  0.4876  1.0000  0.8675  0.4718  0.5341  0.6197  0.7143  0.5605  0.3728  0.2481  0.1921  0.1386  0.3325  0.2883  0.3228  0.2607  0.2040  0.2396  0.1319  0.0683  0.0334  0.0716  0.0976  0.0787  0.0522  0.0500  0.0231  0.0221  0.0144  0.0307  0.0386  0.0147  0.0018  0.0100  0.0096  0.0077  0.0180  0.0109  0.0070  R\n0.0094  0.0166  0.0398  0.0359  0.0681  0.0706  0.1020  0.0893  0.0381  0.1328  0.1303  0.0273  0.0644  0.0712  0.1204  0.0717  0.1224  0.2349  0.3684  0.3918  0.4925  0.8793  0.9606  0.8786  0.6905  0.6937  0.5674  0.6540  0.7802  0.7575  0.5836  0.6316  0.8108  0.9039  0.8647  0.6695  0.4027  0.2370  0.2685  0.3662  0.3267  0.2200  0.2996  0.2205  0.1163  0.0635  0.0465  0.0422  0.0174  0.0172  0.0134  0.0141  0.0191  0.0145  0.0065  0.0129  0.0217  0.0087  0.0077  0.0122  R\n0.0333  0.0221  0.0270  0.0481  0.0679  0.0981  0.0843  0.1172  0.0759  0.0920  0.1475  0.0522  0.1119  0.0970  0.1174  0.1678  0.1642  0.1205  0.0494  0.1544  0.3485  0.6146  0.9146  0.9364  0.8677  0.8772  0.8553  0.8833  1.0000  0.8296  0.6601  0.5499  0.5716  0.6859  0.6825  0.5142  0.2750  0.1358  0.1551  0.2646  0.1994  0.1883  0.2746  0.1651  0.0575  0.0695  0.0598  0.0456  0.0021  0.0068  0.0036  0.0022  0.0032  0.0060  0.0054  0.0063  0.0143  0.0132  0.0051  0.0041  R\n...        ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ... ..\n0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328  0.2684  0.3108  0.2933  0.2275  0.0994  0.1801  0.2200  0.2732  0.2862  0.2034  0.1740  0.4130  0.6879  0.8120  0.8453  0.8919  0.9300  0.9987  1.0000  0.8104  0.6199  0.6041  0.5547  0.4160  0.1472  0.0849  0.0608  0.0969  0.1411  0.1676  0.1200  0.1201  0.1036  0.1977  0.1339  0.0902  0.1085  0.1521  0.1363  0.0858  0.0290  0.0203  0.0116  0.0098  0.0199  0.0033  0.0101  0.0065  0.0115  0.0193  0.0157  M\n0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  0.1018  0.1030  0.2154  0.3085  0.3425  0.2990  0.1402  0.1235  0.1534  0.1901  0.2429  0.2120  0.2395  0.3272  0.5949  0.8302  0.9045  0.9888  0.9912  0.9448  1.0000  0.9092  0.7412  0.7691  0.7117  0.5304  0.2131  0.0928  0.1297  0.1159  0.1226  0.1768  0.0345  0.1562  0.0824  0.1149  0.1694  0.0954  0.0080  0.0790  0.1255  0.0647  0.0179  0.0051  0.0061  0.0093  0.0135  0.0063  0.0063  0.0034  0.0032  0.0062  0.0067  M\n0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258  0.2529  0.2716  0.2374  0.1878  0.0983  0.0683  0.1503  0.1723  0.2339  0.1962  0.1395  0.3164  0.5888  0.7631  0.8473  0.9424  0.9986  0.9699  1.0000  0.8630  0.6979  0.7717  0.7305  0.5197  0.1786  0.1098  0.1446  0.1066  0.1440  0.1929  0.0325  0.1490  0.0328  0.0537  0.1309  0.0910  0.0757  0.1059  0.1005  0.0535  0.0235  0.0155  0.0160  0.0029  0.0051  0.0062  0.0089  0.0140  0.0138  0.0077  0.0031  M\n0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945  0.2354  0.2898  0.2812  0.1578  0.0273  0.0673  0.1444  0.2070  0.2645  0.2828  0.4293  0.5685  0.6990  0.7246  0.7622  0.9242  1.0000  0.9979  0.8297  0.7032  0.7141  0.6893  0.4961  0.2584  0.0969  0.0776  0.0364  0.1572  0.1823  0.1349  0.0849  0.0492  0.1367  0.1552  0.1548  0.1319  0.0985  0.1258  0.0954  0.0489  0.0241  0.0042  0.0086  0.0046  0.0126  0.0036  0.0035  0.0034  0.0079  0.0036  0.0048  M\n0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843  0.2354  0.2720  0.2442  0.1665  0.0336  0.1302  0.1708  0.2177  0.3175  0.3714  0.4552  0.5700  0.7397  0.8062  0.8837  0.9432  1.0000  0.9375  0.7603  0.7123  0.8358  0.7622  0.4567  0.1715  0.1549  0.1641  0.1869  0.2655  0.1713  0.0959  0.0768  0.0847  0.2076  0.2505  0.1862  0.1439  0.1470  0.0991  0.0041  0.0154  0.0116  0.0181  0.0146  0.0129  0.0047  0.0039  0.0061  0.0040  0.0036  0.0061  0.0115  M\n\n[175 rows x 60 columns]"], "text/html": ["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>34</th>\n      <th>35</th>\n      <th>36</th>\n      <th>37</th>\n      <th>38</th>\n      <th>39</th>\n      <th>40</th>\n      <th>41</th>\n      <th>42</th>\n      <th>43</th>\n      <th>44</th>\n      <th>45</th>\n      <th>46</th>\n      <th>47</th>\n      <th>48</th>\n      <th>49</th>\n      <th>50</th>\n      <th>51</th>\n      <th>52</th>\n      <th>53</th>\n      <th>54</th>\n      <th>56</th>\n      <th>57</th>\n      <th>58</th>\n      <th>59</th>\n      <th>60</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0.0442</th>\n      <td>0.0477</td>\n      <td>0.0049</td>\n      <td>0.0581</td>\n      <td>0.0278</td>\n      <td>0.0678</td>\n      <td>0.1664</td>\n      <td>0.1490</td>\n      <td>0.0974</td>\n      <td>0.1268</td>\n      <td>0.1109</td>\n      <td>0.2375</td>\n      <td>0.2007</td>\n      <td>0.2140</td>\n      <td>0.1109</td>\n      <td>0.2036</td>\n      <td>0.2468</td>\n      <td>0.6682</td>\n      <td>0.8345</td>\n      <td>0.8252</td>\n      <td>0.8017</td>\n      <td>0.8982</td>\n      <td>0.9664</td>\n      <td>0.8515</td>\n      <td>0.6626</td>\n      <td>0.3241</td>\n      <td>0.2054</td>\n      <td>0.5669</td>\n      <td>0.5726</td>\n      <td>0.4877</td>\n      <td>0.7532</td>\n      <td>0.7600</td>\n      <td>0.5185</td>\n      <td>0.4120</td>\n      <td>0.5560</td>\n      <td>0.5569</td>\n      <td>0.1336</td>\n      <td>0.3831</td>\n      <td>0.4611</td>\n      <td>0.4330</td>\n      <td>0.2556</td>\n      <td>0.1466</td>\n      <td>0.3489</td>\n      <td>0.2659</td>\n      <td>0.0944</td>\n      <td>0.1370</td>\n      <td>0.1344</td>\n      <td>0.0416</td>\n      <td>0.0719</td>\n      <td>0.0637</td>\n      <td>0.0210</td>\n      <td>0.0204</td>\n      <td>0.0216</td>\n      <td>0.0135</td>\n      <td>0.0055</td>\n      <td>0.0073</td>\n      <td>0.0080</td>\n      <td>0.0105</td>\n      <td>0.0059</td>\n      <td>0.0105</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>0.0311</th>\n      <td>0.0491</td>\n      <td>0.0692</td>\n      <td>0.0831</td>\n      <td>0.0079</td>\n      <td>0.0200</td>\n      <td>0.0981</td>\n      <td>0.1016</td>\n      <td>0.2025</td>\n      <td>0.0767</td>\n      <td>0.1767</td>\n      <td>0.2555</td>\n      <td>0.2812</td>\n      <td>0.2722</td>\n      <td>0.3227</td>\n      <td>0.3463</td>\n      <td>0.5395</td>\n      <td>0.7911</td>\n      <td>0.9064</td>\n      <td>0.8701</td>\n      <td>0.7672</td>\n      <td>0.2957</td>\n      <td>0.4148</td>\n      <td>0.6043</td>\n      <td>0.3178</td>\n      <td>0.3482</td>\n      <td>0.6158</td>\n      <td>0.8049</td>\n      <td>0.6289</td>\n      <td>0.4999</td>\n      <td>0.5830</td>\n      <td>0.6660</td>\n      <td>0.4124</td>\n      <td>0.1260</td>\n      <td>0.2487</td>\n      <td>0.4676</td>\n      <td>0.5382</td>\n      <td>0.3150</td>\n      <td>0.2139</td>\n      <td>0.1848</td>\n      <td>0.1679</td>\n      <td>0.2328</td>\n      <td>0.1015</td>\n      <td>0.0713</td>\n      <td>0.0615</td>\n      <td>0.0779</td>\n      <td>0.0761</td>\n      <td>0.0845</td>\n      <td>0.0592</td>\n      <td>0.0068</td>\n      <td>0.0089</td>\n      <td>0.0087</td>\n      <td>0.0032</td>\n      <td>0.0130</td>\n      <td>0.0188</td>\n      <td>0.0101</td>\n      <td>0.0229</td>\n      <td>0.0182</td>\n      <td>0.0046</td>\n      <td>0.0038</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>0.0206</th>\n      <td>0.0132</td>\n      <td>0.0533</td>\n      <td>0.0569</td>\n      <td>0.0647</td>\n      <td>0.1432</td>\n      <td>0.1344</td>\n      <td>0.2041</td>\n      <td>0.1571</td>\n      <td>0.1573</td>\n      <td>0.2327</td>\n      <td>0.1785</td>\n      <td>0.1507</td>\n      <td>0.1916</td>\n      <td>0.2061</td>\n      <td>0.2307</td>\n      <td>0.2360</td>\n      <td>0.1299</td>\n      <td>0.3812</td>\n      <td>0.5858</td>\n      <td>0.4497</td>\n      <td>0.4876</td>\n      <td>1.0000</td>\n      <td>0.8675</td>\n      <td>0.4718</td>\n      <td>0.5341</td>\n      <td>0.6197</td>\n      <td>0.7143</td>\n      <td>0.5605</td>\n      <td>0.3728</td>\n      <td>0.2481</td>\n      <td>0.1921</td>\n      <td>0.1386</td>\n      <td>0.3325</td>\n      <td>0.2883</td>\n      <td>0.3228</td>\n      <td>0.2607</td>\n      <td>0.2040</td>\n      <td>0.2396</td>\n      <td>0.1319</td>\n      <td>0.0683</td>\n      <td>0.0334</td>\n      <td>0.0716</td>\n      <td>0.0976</td>\n      <td>0.0787</td>\n      <td>0.0522</td>\n      <td>0.0500</td>\n      <td>0.0231</td>\n      <td>0.0221</td>\n      <td>0.0144</td>\n      <td>0.0307</td>\n      <td>0.0386</td>\n      <td>0.0147</td>\n      <td>0.0018</td>\n      <td>0.0100</td>\n      <td>0.0096</td>\n      <td>0.0077</td>\n      <td>0.0180</td>\n      <td>0.0109</td>\n      <td>0.0070</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>0.0094</th>\n      <td>0.0166</td>\n      <td>0.0398</td>\n      <td>0.0359</td>\n      <td>0.0681</td>\n      <td>0.0706</td>\n      <td>0.1020</td>\n      <td>0.0893</td>\n      <td>0.0381</td>\n      <td>0.1328</td>\n      <td>0.1303</td>\n      <td>0.0273</td>\n      <td>0.0644</td>\n      <td>0.0712</td>\n      <td>0.1204</td>\n      <td>0.0717</td>\n      <td>0.1224</td>\n      <td>0.2349</td>\n      <td>0.3684</td>\n      <td>0.3918</td>\n      <td>0.4925</td>\n      <td>0.8793</td>\n      <td>0.9606</td>\n      <td>0.8786</td>\n      <td>0.6905</td>\n      <td>0.6937</td>\n      <td>0.5674</td>\n      <td>0.6540</td>\n      <td>0.7802</td>\n      <td>0.7575</td>\n      <td>0.5836</td>\n      <td>0.6316</td>\n      <td>0.8108</td>\n      <td>0.9039</td>\n      <td>0.8647</td>\n      <td>0.6695</td>\n      <td>0.4027</td>\n      <td>0.2370</td>\n      <td>0.2685</td>\n      <td>0.3662</td>\n      <td>0.3267</td>\n      <td>0.2200</td>\n      <td>0.2996</td>\n      <td>0.2205</td>\n      <td>0.1163</td>\n      <td>0.0635</td>\n      <td>0.0465</td>\n      <td>0.0422</td>\n      <td>0.0174</td>\n      <td>0.0172</td>\n      <td>0.0134</td>\n      <td>0.0141</td>\n      <td>0.0191</td>\n      <td>0.0145</td>\n      <td>0.0065</td>\n      <td>0.0129</td>\n      <td>0.0217</td>\n      <td>0.0087</td>\n      <td>0.0077</td>\n      <td>0.0122</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>0.0333</th>\n      <td>0.0221</td>\n      <td>0.0270</td>\n      <td>0.0481</td>\n      <td>0.0679</td>\n      <td>0.0981</td>\n      <td>0.0843</td>\n      <td>0.1172</td>\n      <td>0.0759</td>\n      <td>0.0920</td>\n      <td>0.1475</td>\n      <td>0.0522</td>\n      <td>0.1119</td>\n      <td>0.0970</td>\n      <td>0.1174</td>\n      <td>0.1678</td>\n      <td>0.1642</td>\n      <td>0.1205</td>\n      <td>0.0494</td>\n      <td>0.1544</td>\n      <td>0.3485</td>\n      <td>0.6146</td>\n      <td>0.9146</td>\n      <td>0.9364</td>\n      <td>0.8677</td>\n      <td>0.8772</td>\n      <td>0.8553</td>\n      <td>0.8833</td>\n      <td>1.0000</td>\n      <td>0.8296</td>\n      <td>0.6601</td>\n      <td>0.5499</td>\n      <td>0.5716</td>\n      <td>0.6859</td>\n      <td>0.6825</td>\n      <td>0.5142</td>\n      <td>0.2750</td>\n      <td>0.1358</td>\n      <td>0.1551</td>\n      <td>0.2646</td>\n      <td>0.1994</td>\n      <td>0.1883</td>\n      <td>0.2746</td>\n      <td>0.1651</td>\n      <td>0.0575</td>\n      <td>0.0695</td>\n      <td>0.0598</td>\n      <td>0.0456</td>\n      <td>0.0021</td>\n      <td>0.0068</td>\n      <td>0.0036</td>\n      <td>0.0022</td>\n      <td>0.0032</td>\n      <td>0.0060</td>\n      <td>0.0054</td>\n      <td>0.0063</td>\n      <td>0.0143</td>\n      <td>0.0132</td>\n      <td>0.0051</td>\n      <td>0.0041</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>0.0187</th>\n      <td>0.0346</td>\n      <td>0.0168</td>\n      <td>0.0177</td>\n      <td>0.0393</td>\n      <td>0.1630</td>\n      <td>0.2028</td>\n      <td>0.1694</td>\n      <td>0.2328</td>\n      <td>0.2684</td>\n      <td>0.3108</td>\n      <td>0.2933</td>\n      <td>0.2275</td>\n      <td>0.0994</td>\n      <td>0.1801</td>\n      <td>0.2200</td>\n      <td>0.2732</td>\n      <td>0.2862</td>\n      <td>0.2034</td>\n      <td>0.1740</td>\n      <td>0.4130</td>\n      <td>0.6879</td>\n      <td>0.8120</td>\n      <td>0.8453</td>\n      <td>0.8919</td>\n      <td>0.9300</td>\n      <td>0.9987</td>\n      <td>1.0000</td>\n      <td>0.8104</td>\n      <td>0.6199</td>\n      <td>0.6041</td>\n      <td>0.5547</td>\n      <td>0.4160</td>\n      <td>0.1472</td>\n      <td>0.0849</td>\n      <td>0.0608</td>\n      <td>0.0969</td>\n      <td>0.1411</td>\n      <td>0.1676</td>\n      <td>0.1200</td>\n      <td>0.1201</td>\n      <td>0.1036</td>\n      <td>0.1977</td>\n      <td>0.1339</td>\n      <td>0.0902</td>\n      <td>0.1085</td>\n      <td>0.1521</td>\n      <td>0.1363</td>\n      <td>0.0858</td>\n      <td>0.0290</td>\n      <td>0.0203</td>\n      <td>0.0116</td>\n      <td>0.0098</td>\n      <td>0.0199</td>\n      <td>0.0033</td>\n      <td>0.0101</td>\n      <td>0.0065</td>\n      <td>0.0115</td>\n      <td>0.0193</td>\n      <td>0.0157</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>0.0323</th>\n      <td>0.0101</td>\n      <td>0.0298</td>\n      <td>0.0564</td>\n      <td>0.0760</td>\n      <td>0.0958</td>\n      <td>0.0990</td>\n      <td>0.1018</td>\n      <td>0.1030</td>\n      <td>0.2154</td>\n      <td>0.3085</td>\n      <td>0.3425</td>\n      <td>0.2990</td>\n      <td>0.1402</td>\n      <td>0.1235</td>\n      <td>0.1534</td>\n      <td>0.1901</td>\n      <td>0.2429</td>\n      <td>0.2120</td>\n      <td>0.2395</td>\n      <td>0.3272</td>\n      <td>0.5949</td>\n      <td>0.8302</td>\n      <td>0.9045</td>\n      <td>0.9888</td>\n      <td>0.9912</td>\n      <td>0.9448</td>\n      <td>1.0000</td>\n      <td>0.9092</td>\n      <td>0.7412</td>\n      <td>0.7691</td>\n      <td>0.7117</td>\n      <td>0.5304</td>\n      <td>0.2131</td>\n      <td>0.0928</td>\n      <td>0.1297</td>\n      <td>0.1159</td>\n      <td>0.1226</td>\n      <td>0.1768</td>\n      <td>0.0345</td>\n      <td>0.1562</td>\n      <td>0.0824</td>\n      <td>0.1149</td>\n      <td>0.1694</td>\n      <td>0.0954</td>\n      <td>0.0080</td>\n      <td>0.0790</td>\n      <td>0.1255</td>\n      <td>0.0647</td>\n      <td>0.0179</td>\n      <td>0.0051</td>\n      <td>0.0061</td>\n      <td>0.0093</td>\n      <td>0.0135</td>\n      <td>0.0063</td>\n      <td>0.0063</td>\n      <td>0.0034</td>\n      <td>0.0032</td>\n      <td>0.0062</td>\n      <td>0.0067</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>0.0522</th>\n      <td>0.0437</td>\n      <td>0.0180</td>\n      <td>0.0292</td>\n      <td>0.0351</td>\n      <td>0.1171</td>\n      <td>0.1257</td>\n      <td>0.1178</td>\n      <td>0.1258</td>\n      <td>0.2529</td>\n      <td>0.2716</td>\n      <td>0.2374</td>\n      <td>0.1878</td>\n      <td>0.0983</td>\n      <td>0.0683</td>\n      <td>0.1503</td>\n      <td>0.1723</td>\n      <td>0.2339</td>\n      <td>0.1962</td>\n      <td>0.1395</td>\n      <td>0.3164</td>\n      <td>0.5888</td>\n      <td>0.7631</td>\n      <td>0.8473</td>\n      <td>0.9424</td>\n      <td>0.9986</td>\n      <td>0.9699</td>\n      <td>1.0000</td>\n      <td>0.8630</td>\n      <td>0.6979</td>\n      <td>0.7717</td>\n      <td>0.7305</td>\n      <td>0.5197</td>\n      <td>0.1786</td>\n      <td>0.1098</td>\n      <td>0.1446</td>\n      <td>0.1066</td>\n      <td>0.1440</td>\n      <td>0.1929</td>\n      <td>0.0325</td>\n      <td>0.1490</td>\n      <td>0.0328</td>\n      <td>0.0537</td>\n      <td>0.1309</td>\n      <td>0.0910</td>\n      <td>0.0757</td>\n      <td>0.1059</td>\n      <td>0.1005</td>\n      <td>0.0535</td>\n      <td>0.0235</td>\n      <td>0.0155</td>\n      <td>0.0160</td>\n      <td>0.0029</td>\n      <td>0.0051</td>\n      <td>0.0062</td>\n      <td>0.0089</td>\n      <td>0.0140</td>\n      <td>0.0138</td>\n      <td>0.0077</td>\n      <td>0.0031</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>0.0303</th>\n      <td>0.0353</td>\n      <td>0.0490</td>\n      <td>0.0608</td>\n      <td>0.0167</td>\n      <td>0.1354</td>\n      <td>0.1465</td>\n      <td>0.1123</td>\n      <td>0.1945</td>\n      <td>0.2354</td>\n      <td>0.2898</td>\n      <td>0.2812</td>\n      <td>0.1578</td>\n      <td>0.0273</td>\n      <td>0.0673</td>\n      <td>0.1444</td>\n      <td>0.2070</td>\n      <td>0.2645</td>\n      <td>0.2828</td>\n      <td>0.4293</td>\n      <td>0.5685</td>\n      <td>0.6990</td>\n      <td>0.7246</td>\n      <td>0.7622</td>\n      <td>0.9242</td>\n      <td>1.0000</td>\n      <td>0.9979</td>\n      <td>0.8297</td>\n      <td>0.7032</td>\n      <td>0.7141</td>\n      <td>0.6893</td>\n      <td>0.4961</td>\n      <td>0.2584</td>\n      <td>0.0969</td>\n      <td>0.0776</td>\n      <td>0.0364</td>\n      <td>0.1572</td>\n      <td>0.1823</td>\n      <td>0.1349</td>\n      <td>0.0849</td>\n      <td>0.0492</td>\n      <td>0.1367</td>\n      <td>0.1552</td>\n      <td>0.1548</td>\n      <td>0.1319</td>\n      <td>0.0985</td>\n      <td>0.1258</td>\n      <td>0.0954</td>\n      <td>0.0489</td>\n      <td>0.0241</td>\n      <td>0.0042</td>\n      <td>0.0086</td>\n      <td>0.0046</td>\n      <td>0.0126</td>\n      <td>0.0036</td>\n      <td>0.0035</td>\n      <td>0.0034</td>\n      <td>0.0079</td>\n      <td>0.0036</td>\n      <td>0.0048</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>0.0260</th>\n      <td>0.0363</td>\n      <td>0.0136</td>\n      <td>0.0272</td>\n      <td>0.0214</td>\n      <td>0.0338</td>\n      <td>0.0655</td>\n      <td>0.1400</td>\n      <td>0.1843</td>\n      <td>0.2354</td>\n      <td>0.2720</td>\n      <td>0.2442</td>\n      <td>0.1665</td>\n      <td>0.0336</td>\n      <td>0.1302</td>\n      <td>0.1708</td>\n      <td>0.2177</td>\n      <td>0.3175</td>\n      <td>0.3714</td>\n      <td>0.4552</td>\n      <td>0.5700</td>\n      <td>0.7397</td>\n      <td>0.8062</td>\n      <td>0.8837</td>\n      <td>0.9432</td>\n      <td>1.0000</td>\n      <td>0.9375</td>\n      <td>0.7603</td>\n      <td>0.7123</td>\n      <td>0.8358</td>\n      <td>0.7622</td>\n      <td>0.4567</td>\n      <td>0.1715</td>\n      <td>0.1549</td>\n      <td>0.1641</td>\n      <td>0.1869</td>\n      <td>0.2655</td>\n      <td>0.1713</td>\n      <td>0.0959</td>\n      <td>0.0768</td>\n      <td>0.0847</td>\n      <td>0.2076</td>\n      <td>0.2505</td>\n      <td>0.1862</td>\n      <td>0.1439</td>\n      <td>0.1470</td>\n      <td>0.0991</td>\n      <td>0.0041</td>\n      <td>0.0154</td>\n      <td>0.0116</td>\n      <td>0.0181</td>\n      <td>0.0146</td>\n      <td>0.0129</td>\n      <td>0.0047</td>\n      <td>0.0039</td>\n      <td>0.0061</td>\n      <td>0.0040</td>\n      <td>0.0036</td>\n      <td>0.0061</td>\n      <td>0.0115</td>\n      <td>M</td>\n    </tr>\n  </tbody>\n</table>\n<p>175 rows \u00d7 60 columns</p>\n</div>"]}, "metadata": {}, "execution_count": 1, "output_type": "execute_result"}]}, {"cell_type": "code", "metadata": {"id": "0_0.6083523399607911"}, "execution_count": 1, "source": ["# No of examples\ndf.shape"], "outputs": [{"data": {"text/plain": "(175, 60)"}, "metadata": {}, "execution_count": 2, "output_type": "execute_result"}]}, {"cell_type": "code", "metadata": {"id": "0_0.4922246147752434"}, "execution_count": 2, "source": ["# Descriptive analysis of the data\ndf.describe()"], "outputs": [{"data": {"text/plain": ["                0           1           2           3           4           5           6           7           8           9          10          11          12          13          14          15          16          17          18          19          20          21          22          23          24          25          26          27          28          29          30          31          32          33          34          35          36          37          38          39          40          41          42          43          44          45          46          47          48          49          50          51          52          53          54          56          57          58          59\ncount  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000\nmean     0.039099    0.043781    0.054401    0.075051    0.104713    0.121982    0.136359    0.181121    0.209518    0.241015    0.251227    0.270678    0.298474    0.320623    0.372799    0.409113    0.447809    0.502383    0.561974    0.618351    0.637118    0.662147    0.692741    0.692774    0.717834    0.721132    0.709700    0.656487    0.593337    0.507393    0.436463    0.413743    0.398240    0.385444    0.370943    0.347531    0.327921    0.318503    0.305262    0.284508    0.280011    0.254199    0.219402    0.203865    0.163854    0.125604    0.094738    0.054166    0.020372    0.016258    0.013643    0.010694    0.010978    0.009119    0.008230    0.007662    0.008065    0.008019    0.006615\nstd      0.034592    0.039751    0.047694    0.055420    0.055374    0.059362    0.081155    0.115280    0.131929    0.129329    0.134464    0.136521    0.159073    0.201921    0.232076    0.263885    0.266479    0.263131    0.264012    0.257938    0.258085    0.254447    0.237271    0.245233    0.236203    0.240157    0.230954    0.235359    0.210756    0.209113    0.207692    0.199758    0.224776    0.261010    0.268594    0.238564    0.201827    0.188519    0.166054    0.163815    0.167152    0.142614    0.139410    0.159849    0.138279    0.087932    0.063697    0.035953    0.013500    0.012393    0.009908    0.007171    0.007545    0.007382    0.005774    0.005694    0.006680    0.006468    0.005334\nmin      0.000600    0.001500    0.005800    0.006700    0.011600    0.003300    0.005500    0.011700    0.011300    0.028900    0.023600    0.025200    0.027300    0.009200    0.042200    0.036700    0.037500    0.049400    0.074000    0.051200    0.021900    0.056300    0.023900    0.024000    0.092100    0.048100    0.028400    0.014400    0.082300    0.048200    0.077300    0.047700    0.021200    0.022300    0.008000    0.035100    0.038300    0.037100    0.011700    0.036000    0.005600    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000800    0.000500    0.001000    0.000600    0.000400    0.000300    0.000600    0.000100    0.000600\n25%      0.016550    0.018450    0.024350    0.039200    0.069000    0.084800    0.086700    0.105850    0.116300    0.144350    0.137950    0.165750    0.183100    0.164250    0.190500    0.203300    0.238300    0.298950    0.337500    0.411600    0.418050    0.473600    0.582150    0.574500    0.562800    0.566200    0.573500    0.471650    0.427950    0.349300    0.287850    0.257350    0.215300    0.170350    0.146150    0.157600    0.174250    0.176500    0.182950    0.160600    0.160250    0.155450    0.128150    0.090950    0.068400    0.066750    0.046250    0.028100    0.011500    0.008550    0.007350    0.004900    0.005350    0.003850    0.004400    0.003700    0.003600    0.003550    0.003000\n50%      0.029700    0.034200    0.044500    0.063000    0.093200    0.110800    0.113000    0.158300    0.184700    0.234000    0.255500    0.262400    0.282900    0.279400    0.295600    0.293400    0.342400    0.432700    0.549200    0.652700    0.698800    0.724600    0.736000    0.751900    0.782500    0.765200    0.736000    0.703200    0.615100    0.490100    0.424100    0.393900    0.376300    0.309300    0.289700    0.269900    0.287100    0.278900    0.277100    0.257500    0.254600    0.238000    0.177900    0.149900    0.118100    0.103400    0.084500    0.047500    0.017900    0.014000    0.011700    0.009300    0.009300    0.007200    0.006800    0.005800    0.006000    0.006300    0.005300\n75%      0.047300    0.055050    0.065700    0.099400    0.134150    0.153050    0.169800    0.236300    0.267800    0.306700    0.328600    0.335800    0.390000    0.452650    0.537950    0.640050    0.676600    0.750650    0.809750    0.826850    0.841900    0.866550    0.878700    0.889800    0.909550    0.925650    0.908850    0.856050    0.731350    0.643200    0.555250    0.539700    0.565350    0.591400    0.541500    0.481350    0.418500    0.427650    0.416950    0.389150    0.387800    0.341050    0.279500    0.239300    0.216050    0.163750    0.126900    0.073000    0.025450    0.021150    0.016750    0.015000    0.014450    0.012200    0.010350    0.010200    0.010500    0.010350    0.008700\nmax      0.233900    0.305900    0.426400    0.401000    0.382300    0.372900    0.459000    0.682800    0.710600    0.734200    0.577100    0.713100    0.997000    0.913700    0.975100    1.000000    1.000000    1.000000    1.000000    1.000000    1.000000    1.000000    1.000000    1.000000    1.000000    1.000000    1.000000    1.000000    1.000000    0.965700    0.930600    1.000000    0.953600    1.000000    1.000000    0.949700    0.948000    0.884900    0.897900    0.775100    0.824600    0.773300    0.776200    0.703400    0.729200    0.552200    0.333900    0.179400    0.082500    0.100400    0.070900    0.039000    0.035200    0.044700    0.039400    0.035500    0.044000    0.036400    0.043900"], "text/html": ["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>34</th>\n      <th>35</th>\n      <th>36</th>\n      <th>37</th>\n      <th>38</th>\n      <th>39</th>\n      <th>40</th>\n      <th>41</th>\n      <th>42</th>\n      <th>43</th>\n      <th>44</th>\n      <th>45</th>\n      <th>46</th>\n      <th>47</th>\n      <th>48</th>\n      <th>49</th>\n      <th>50</th>\n      <th>51</th>\n      <th>52</th>\n      <th>53</th>\n      <th>54</th>\n      <th>56</th>\n      <th>57</th>\n      <th>58</th>\n      <th>59</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n      <td>175.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.039099</td>\n      <td>0.043781</td>\n      <td>0.054401</td>\n      <td>0.075051</td>\n      <td>0.104713</td>\n      <td>0.121982</td>\n      <td>0.136359</td>\n      <td>0.181121</td>\n      <td>0.209518</td>\n      <td>0.241015</td>\n      <td>0.251227</td>\n      <td>0.270678</td>\n      <td>0.298474</td>\n      <td>0.320623</td>\n      <td>0.372799</td>\n      <td>0.409113</td>\n      <td>0.447809</td>\n      <td>0.502383</td>\n      <td>0.561974</td>\n      <td>0.618351</td>\n      <td>0.637118</td>\n      <td>0.662147</td>\n      <td>0.692741</td>\n      <td>0.692774</td>\n      <td>0.717834</td>\n      <td>0.721132</td>\n      <td>0.709700</td>\n      <td>0.656487</td>\n      <td>0.593337</td>\n      <td>0.507393</td>\n      <td>0.436463</td>\n      <td>0.413743</td>\n      <td>0.398240</td>\n      <td>0.385444</td>\n      <td>0.370943</td>\n      <td>0.347531</td>\n      <td>0.327921</td>\n      <td>0.318503</td>\n      <td>0.305262</td>\n      <td>0.284508</td>\n      <td>0.280011</td>\n      <td>0.254199</td>\n      <td>0.219402</td>\n      <td>0.203865</td>\n      <td>0.163854</td>\n      <td>0.125604</td>\n      <td>0.094738</td>\n      <td>0.054166</td>\n      <td>0.020372</td>\n      <td>0.016258</td>\n      <td>0.013643</td>\n      <td>0.010694</td>\n      <td>0.010978</td>\n      <td>0.009119</td>\n      <td>0.008230</td>\n      <td>0.007662</td>\n      <td>0.008065</td>\n      <td>0.008019</td>\n      <td>0.006615</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.034592</td>\n      <td>0.039751</td>\n      <td>0.047694</td>\n      <td>0.055420</td>\n      <td>0.055374</td>\n      <td>0.059362</td>\n      <td>0.081155</td>\n      <td>0.115280</td>\n      <td>0.131929</td>\n      <td>0.129329</td>\n      <td>0.134464</td>\n      <td>0.136521</td>\n      <td>0.159073</td>\n      <td>0.201921</td>\n      <td>0.232076</td>\n      <td>0.263885</td>\n      <td>0.266479</td>\n      <td>0.263131</td>\n      <td>0.264012</td>\n      <td>0.257938</td>\n      <td>0.258085</td>\n      <td>0.254447</td>\n      <td>0.237271</td>\n      <td>0.245233</td>\n      <td>0.236203</td>\n      <td>0.240157</td>\n      <td>0.230954</td>\n      <td>0.235359</td>\n      <td>0.210756</td>\n      <td>0.209113</td>\n      <td>0.207692</td>\n      <td>0.199758</td>\n      <td>0.224776</td>\n      <td>0.261010</td>\n      <td>0.268594</td>\n      <td>0.238564</td>\n      <td>0.201827</td>\n      <td>0.188519</td>\n      <td>0.166054</td>\n      <td>0.163815</td>\n      <td>0.167152</td>\n      <td>0.142614</td>\n      <td>0.139410</td>\n      <td>0.159849</td>\n      <td>0.138279</td>\n      <td>0.087932</td>\n      <td>0.063697</td>\n      <td>0.035953</td>\n      <td>0.013500</td>\n      <td>0.012393</td>\n      <td>0.009908</td>\n      <td>0.007171</td>\n      <td>0.007545</td>\n      <td>0.007382</td>\n      <td>0.005774</td>\n      <td>0.005694</td>\n      <td>0.006680</td>\n      <td>0.006468</td>\n      <td>0.005334</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000600</td>\n      <td>0.001500</td>\n      <td>0.005800</td>\n      <td>0.006700</td>\n      <td>0.011600</td>\n      <td>0.003300</td>\n      <td>0.005500</td>\n      <td>0.011700</td>\n      <td>0.011300</td>\n      <td>0.028900</td>\n      <td>0.023600</td>\n      <td>0.025200</td>\n      <td>0.027300</td>\n      <td>0.009200</td>\n      <td>0.042200</td>\n      <td>0.036700</td>\n      <td>0.037500</td>\n      <td>0.049400</td>\n      <td>0.074000</td>\n      <td>0.051200</td>\n      <td>0.021900</td>\n      <td>0.056300</td>\n      <td>0.023900</td>\n      <td>0.024000</td>\n      <td>0.092100</td>\n      <td>0.048100</td>\n      <td>0.028400</td>\n      <td>0.014400</td>\n      <td>0.082300</td>\n      <td>0.048200</td>\n      <td>0.077300</td>\n      <td>0.047700</td>\n      <td>0.021200</td>\n      <td>0.022300</td>\n      <td>0.008000</td>\n      <td>0.035100</td>\n      <td>0.038300</td>\n      <td>0.037100</td>\n      <td>0.011700</td>\n      <td>0.036000</td>\n      <td>0.005600</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000800</td>\n      <td>0.000500</td>\n      <td>0.001000</td>\n      <td>0.000600</td>\n      <td>0.000400</td>\n      <td>0.000300</td>\n      <td>0.000600</td>\n      <td>0.000100</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.016550</td>\n      <td>0.018450</td>\n      <td>0.024350</td>\n      <td>0.039200</td>\n      <td>0.069000</td>\n      <td>0.084800</td>\n      <td>0.086700</td>\n      <td>0.105850</td>\n      <td>0.116300</td>\n      <td>0.144350</td>\n      <td>0.137950</td>\n      <td>0.165750</td>\n      <td>0.183100</td>\n      <td>0.164250</td>\n      <td>0.190500</td>\n      <td>0.203300</td>\n      <td>0.238300</td>\n      <td>0.298950</td>\n      <td>0.337500</td>\n      <td>0.411600</td>\n      <td>0.418050</td>\n      <td>0.473600</td>\n      <td>0.582150</td>\n      <td>0.574500</td>\n      <td>0.562800</td>\n      <td>0.566200</td>\n      <td>0.573500</td>\n      <td>0.471650</td>\n      <td>0.427950</td>\n      <td>0.349300</td>\n      <td>0.287850</td>\n      <td>0.257350</td>\n      <td>0.215300</td>\n      <td>0.170350</td>\n      <td>0.146150</td>\n      <td>0.157600</td>\n      <td>0.174250</td>\n      <td>0.176500</td>\n      <td>0.182950</td>\n      <td>0.160600</td>\n      <td>0.160250</td>\n      <td>0.155450</td>\n      <td>0.128150</td>\n      <td>0.090950</td>\n      <td>0.068400</td>\n      <td>0.066750</td>\n      <td>0.046250</td>\n      <td>0.028100</td>\n      <td>0.011500</td>\n      <td>0.008550</td>\n      <td>0.007350</td>\n      <td>0.004900</td>\n      <td>0.005350</td>\n      <td>0.003850</td>\n      <td>0.004400</td>\n      <td>0.003700</td>\n      <td>0.003600</td>\n      <td>0.003550</td>\n      <td>0.003000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.029700</td>\n      <td>0.034200</td>\n      <td>0.044500</td>\n      <td>0.063000</td>\n      <td>0.093200</td>\n      <td>0.110800</td>\n      <td>0.113000</td>\n      <td>0.158300</td>\n      <td>0.184700</td>\n      <td>0.234000</td>\n      <td>0.255500</td>\n      <td>0.262400</td>\n      <td>0.282900</td>\n      <td>0.279400</td>\n      <td>0.295600</td>\n      <td>0.293400</td>\n      <td>0.342400</td>\n      <td>0.432700</td>\n      <td>0.549200</td>\n      <td>0.652700</td>\n      <td>0.698800</td>\n      <td>0.724600</td>\n      <td>0.736000</td>\n      <td>0.751900</td>\n      <td>0.782500</td>\n      <td>0.765200</td>\n      <td>0.736000</td>\n      <td>0.703200</td>\n      <td>0.615100</td>\n      <td>0.490100</td>\n      <td>0.424100</td>\n      <td>0.393900</td>\n      <td>0.376300</td>\n      <td>0.309300</td>\n      <td>0.289700</td>\n      <td>0.269900</td>\n      <td>0.287100</td>\n      <td>0.278900</td>\n      <td>0.277100</td>\n      <td>0.257500</td>\n      <td>0.254600</td>\n      <td>0.238000</td>\n      <td>0.177900</td>\n      <td>0.149900</td>\n      <td>0.118100</td>\n      <td>0.103400</td>\n      <td>0.084500</td>\n      <td>0.047500</td>\n      <td>0.017900</td>\n      <td>0.014000</td>\n      <td>0.011700</td>\n      <td>0.009300</td>\n      <td>0.009300</td>\n      <td>0.007200</td>\n      <td>0.006800</td>\n      <td>0.005800</td>\n      <td>0.006000</td>\n      <td>0.006300</td>\n      <td>0.005300</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.047300</td>\n      <td>0.055050</td>\n      <td>0.065700</td>\n      <td>0.099400</td>\n      <td>0.134150</td>\n      <td>0.153050</td>\n      <td>0.169800</td>\n      <td>0.236300</td>\n      <td>0.267800</td>\n      <td>0.306700</td>\n      <td>0.328600</td>\n      <td>0.335800</td>\n      <td>0.390000</td>\n      <td>0.452650</td>\n      <td>0.537950</td>\n      <td>0.640050</td>\n      <td>0.676600</td>\n      <td>0.750650</td>\n      <td>0.809750</td>\n      <td>0.826850</td>\n      <td>0.841900</td>\n      <td>0.866550</td>\n      <td>0.878700</td>\n      <td>0.889800</td>\n      <td>0.909550</td>\n      <td>0.925650</td>\n      <td>0.908850</td>\n      <td>0.856050</td>\n      <td>0.731350</td>\n      <td>0.643200</td>\n      <td>0.555250</td>\n      <td>0.539700</td>\n      <td>0.565350</td>\n      <td>0.591400</td>\n      <td>0.541500</td>\n      <td>0.481350</td>\n      <td>0.418500</td>\n      <td>0.427650</td>\n      <td>0.416950</td>\n      <td>0.389150</td>\n      <td>0.387800</td>\n      <td>0.341050</td>\n      <td>0.279500</td>\n      <td>0.239300</td>\n      <td>0.216050</td>\n      <td>0.163750</td>\n      <td>0.126900</td>\n      <td>0.073000</td>\n      <td>0.025450</td>\n      <td>0.021150</td>\n      <td>0.016750</td>\n      <td>0.015000</td>\n      <td>0.014450</td>\n      <td>0.012200</td>\n      <td>0.010350</td>\n      <td>0.010200</td>\n      <td>0.010500</td>\n      <td>0.010350</td>\n      <td>0.008700</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.233900</td>\n      <td>0.305900</td>\n      <td>0.426400</td>\n      <td>0.401000</td>\n      <td>0.382300</td>\n      <td>0.372900</td>\n      <td>0.459000</td>\n      <td>0.682800</td>\n      <td>0.710600</td>\n      <td>0.734200</td>\n      <td>0.577100</td>\n      <td>0.713100</td>\n      <td>0.997000</td>\n      <td>0.913700</td>\n      <td>0.975100</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.965700</td>\n      <td>0.930600</td>\n      <td>1.000000</td>\n      <td>0.953600</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.949700</td>\n      <td>0.948000</td>\n      <td>0.884900</td>\n      <td>0.897900</td>\n      <td>0.775100</td>\n      <td>0.824600</td>\n      <td>0.773300</td>\n      <td>0.776200</td>\n      <td>0.703400</td>\n      <td>0.729200</td>\n      <td>0.552200</td>\n      <td>0.333900</td>\n      <td>0.179400</td>\n      <td>0.082500</td>\n      <td>0.100400</td>\n      <td>0.070900</td>\n      <td>0.039000</td>\n      <td>0.035200</td>\n      <td>0.044700</td>\n      <td>0.039400</td>\n      <td>0.035500</td>\n      <td>0.044000</td>\n      <td>0.036400</td>\n      <td>0.043900</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}, "metadata": {}, "execution_count": 3, "output_type": "execute_result"}]}, {"cell_type": "code", "metadata": {"id": "0_0.31219478432667547"}, "execution_count": 3, "source": ["df['60'].value_counts()"], "outputs": [{"data": {"text/plain": "60\nM    111\nR     64\nName: count, dtype: int64"}, "metadata": {}, "execution_count": 4, "output_type": "execute_result"}]}, {"cell_type": "code", "metadata": {"id": "0_0.4233446556788296"}, "execution_count": 4, "source": ["# Count Plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.countplot(x='60', data=df)\nplt.title('Distribution of Classes')\nplt.show()"], "outputs": [{"data": {"text/plain": ["<Figure size 640x480 with 1 Axes>"], "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHHCAYAAABZbpmkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAquklEQVR4nO3de3zMd77H8fckkUkqEpcySSoiB61LU2z1aNC6ND2htHVqqT7oupXdlipRNA6x1OXQJZZFsC2q7XaxRWvXbePWqnuv6halKE3SVpMhJEF+548+zOk0LpGMzOTr9Xw85vHY+f5+85tP+4h67W9+v4nNsixLAAAAhvLz9gAAAAC3ErEDAACMRuwAAACjETsAAMBoxA4AADAasQMAAIxG7AAAAKMROwAAwGjEDgAAMBqxA5QTf/zjH2Wz2crkvdq0aaM2bdq4nm/evFk2m03Lly8vk/fv3bu3ateuXSbvVVLnzp3Tc889p/DwcNlsNg0ZMsQjx120aJFsNpu++eYbjxwPALEDeMWVv9CuPIKCghQZGamEhATNnDlTZ8+e9cj7nD59Wn/84x/12WefeeR4nuTLsxXHpEmTtGjRIj3//PNasmSJnn322evuf/nyZS1cuFBt2rRR1apVZbfbVbt2bfXp00d79uwpo6mB21OAtwcAbmfjx49XTEyMLl68qIyMDG3evFlDhgzR9OnT9f777+u+++5z7Tt69Gi98sorN3X806dPa9y4capdu7aaNGlS7NetX7/+pt6nJK4324IFC1RYWHjLZyiNjRs36sEHH9TYsWNvuO+FCxf01FNPae3atXr44Yc1atQoVa1aVd98842WLl2qxYsX68SJE6pZs2YZTA7cfogdwIs6dOigZs2auZ4nJSVp48aN6tSpk5544gkdOHBAwcHBkqSAgAAFBNzaP7Lnz5/XHXfcocDAwFv6PjdSoUIFr75/cWRlZalhw4bF2nf48OFau3atUlJSinzcNXbsWKWkpNyCCQFcwcdYgI9p166dxowZo+PHj+utt95yrV/tmp0NGzaoVatWqly5skJCQnTPPfdo1KhRkn6+zuaBBx6QJPXp08f1kdmiRYsk/Xxdzr333qu9e/fq4Ycf1h133OF67a+v2bni8uXLGjVqlMLDw1WxYkU98cQTOnnypNs+tWvXVu/evYu89pfHvNFsV7tmJzc3V8OGDVNUVJTsdrvuuece/elPf5JlWW772Ww2DRo0SCtXrtS9994ru92uRo0aae3atVf/F/4rWVlZ6tevnxwOh4KCgtS4cWMtXrzYtf3K9UvHjh3TP//5T9fs17rG5ttvv9W8efP06KOPXvW6Hn9/f7388svXPauzatUqdezYUZGRkbLb7apTp45effVVXb582W2/9PR0denSReHh4QoKClLNmjXVvXt35eTkuPa53s/MFfn5+Ro7dqzq1q0ru92uqKgojRgxQvn5+W77FedYgC/gzA7gg5599lmNGjVK69evV//+/a+6z1dffaVOnTrpvvvu0/jx42W323XkyBFt27ZNktSgQQONHz9eycnJGjBggB566CFJUosWLVzH+PHHH9WhQwd1795dPXv2lMPhuO5cEydOlM1m08iRI5WVlaUZM2YoPj5en332mesMVHEUZ7ZfsixLTzzxhDZt2qR+/fqpSZMmWrdunYYPH65Tp04VOTPy0Ucf6b333tMLL7ygSpUqaebMmerSpYtOnDihatWqXXOuCxcuqE2bNjpy5IgGDRqkmJgYLVu2TL1791Z2drZeeuklNWjQQEuWLNHQoUNVs2ZNDRs2TJJUvXr1qx5zzZo1unTp0g2v6bmeRYsWKSQkRImJiQoJCdHGjRuVnJwsp9Op1157TZJUUFCghIQE5efn68UXX1R4eLhOnTql1atXKzs7W2FhYTf8mZGkwsJCPfHEE/roo480YMAANWjQQF9++aVSUlJ0+PBhrVy5UtKNf/4An2IBKHMLFy60JFm7d+++5j5hYWFW06ZNXc/Hjh1r/fKPbEpKiiXJ+v777695jN27d1uSrIULFxbZ1rp1a0uSlZqaetVtrVu3dj3ftGmTJcm66667LKfT6VpfunSpJcn685//7FqLjo62evXqdcNjXm+2Xr16WdHR0a7nK1eutCRZEyZMcNvvt7/9rWWz2awjR4641iRZgYGBbmuff/65JcmaNWtWkff6pRkzZliSrLfeesu1VlBQYMXFxVkhISFu/+zR0dFWx44dr3s8y7KsoUOHWpKsTz/99Ib7Wtb//2wcO3bMtXb+/Pki+/3+97+37rjjDisvL8+yLMv69NNPLUnWsmXLrnns4vzMLFmyxPLz87M+/PBDt/XU1FRLkrVt27ZiHwvwFXyMBfiokJCQ696VVblyZUk/f8RR0ot57Xa7+vTpU+z9f/e736lSpUqu57/97W8VERGhf/3rXyV6/+L617/+JX9/fw0ePNhtfdiwYbIsS2vWrHFbj4+PV506dVzP77vvPoWGhuro0aM3fJ/w8HA988wzrrUKFSpo8ODBOnfunLZs2XLTszudTkly+/d2s3551uzs2bP64Ycf9NBDD+n8+fM6ePCgJCksLEyStG7dOp0/f/6qxynOz8yyZcvUoEED1a9fXz/88IPr0a5dO0nSpk2bin0swFcQO4CPOnfu3HX/gnz66afVsmVLPffcc3I4HOrevbuWLl16U3/x3HXXXTd1MXK9evXcnttsNtWtW/eWfyfM8ePHFRkZWeTfR4MGDVzbf6lWrVpFjlGlShX99NNPN3yfevXqyc/P/T+N13qf4ggNDZWkUn2dwFdffaX//u//VlhYmEJDQ1W9enX17NlTklzX48TExCgxMVF//etfdeeddyohIUGzZ892u16nOD8z6enp+uqrr1S9enW3x9133y3p52uainsswFcQO4AP+vbbb5WTk6O6detec5/g4GBt3bpV//73v/Xss8/qiy++0NNPP61HH320yIWr1zuGp13riw+LO5Mn+Pv7X3Xd+tXFzGWhfv36kqQvv/yyRK/Pzs5W69at9fnnn2v8+PH64IMPtGHDBk2ZMkWS3OJi2rRp+uKLLzRq1ChduHBBgwcPVqNGjfTtt99KKt7PTGFhoWJjY7Vhw4arPl544YViHwvwFcQO4IOWLFkiSUpISLjufn5+fnrkkUc0ffp07d+/XxMnTtTGjRtdHzV4+huX09PT3Z5blqUjR4643TlVpUoVZWdnF3ntr8+K3Mxs0dHROn36dJGzI1c+womOji72sW70Punp6UXOTpTmfTp06CB/f3+3O+tuxubNm/Xjjz9q0aJFeumll9SpUyfFx8erSpUqV90/NjZWo0eP1tatW/Xhhx/q1KlTSk1NdW2/0c9MnTp1dObMGT3yyCOKj48v8rjnnnuKfSzAVxA7gI/ZuHGjXn31VcXExKhHjx7X3O/MmTNF1q58Od+VW4QrVqwoSVeNj5J488033YJj+fLl+u6779ShQwfXWp06dbRjxw4VFBS41lavXl3kFvWbme2xxx7T5cuX9Ze//MVtPSUlRTabze39S+Oxxx5TRkaG/v73v7vWLl26pFmzZikkJEStW7e+6WNGRUWpf//+Wr9+vWbNmlVke2FhoaZNm+Y6+/JrV85S/fKsVEFBgebMmeO2n9Pp1KVLl9zWYmNj5efn5/p5KM7PTLdu3XTq1CktWLCgyL4XLlxQbm5usY8F+ApuPQe8aM2aNTp48KAuXbqkzMxMbdy4URs2bFB0dLTef/99BQUFXfO148eP19atW9WxY0dFR0crKytLc+bMUc2aNdWqVStJP4dH5cqVlZqaqkqVKqlixYpq3ry5YmJiSjRv1apV1apVK/Xp00eZmZmaMWOG6tat63Z7/HPPPafly5erffv26tatm77++mu99dZbbhcM3+xsjz/+uNq2bav/+Z//0TfffKPGjRtr/fr1WrVqlYYMGVLk2CU1YMAAzZs3T71799bevXtVu3ZtLV++XNu2bdOMGTNKfJHxtGnT9PXXX2vw4MF677331KlTJ1WpUkUnTpzQsmXLdPDgQXXv3v2qr23RooWqVKmiXr16afDgwbLZbFqyZEmRj+Q2btyoQYMGqWvXrrr77rt16dIlLVmyRP7+/urSpYuk4v3MPPvss1q6dKn+8Ic/aNOmTWrZsqUuX76sgwcPaunSpVq3bp2aNWtWrGMBPsOr94IBt6krtxdfeQQGBlrh4eHWo48+av35z392u8X5il/fep6WlmY9+eSTVmRkpBUYGGhFRkZazzzzjHX48GG3161atcpq2LChFRAQ4Hard+vWra1GjRpddb5r3Xr+t7/9zUpKSrJq1KhhBQcHWx07drSOHz9e5PXTpk2z7rrrLstut1stW7a09uzZU+SY15vt17eeW5ZlnT171ho6dKgVGRlpVahQwapXr5712muvWYWFhW77SbIGDhxYZKZr3RL/a5mZmVafPn2sO++80woMDLRiY2Ovent8cW89v+LSpUvWX//6V+uhhx6ywsLCrAoVKljR0dFWnz593G5Lv9qt59u2bbMefPBBKzg42IqMjLRGjBhhrVu3zpJkbdq0ybIsyzp69KjVt29fq06dOlZQUJBVtWpVq23btta///1v13GK+zNTUFBgTZkyxWrUqJFlt9utKlWqWPfff781btw4Kycn56aOBfgCm2V54Yo9AACAMsI1OwAAwGjEDgAAMBqxAwAAjEbsAAAAoxE7AADAaMQOAAAwGl8qqJ+/wfT06dOqVKmSx79eHwAA3BqWZens2bOKjIws8gt8f4nYkXT69GlFRUV5ewwAAFACJ0+eVM2aNa+5ndiRXF8Bf/LkSYWGhnp5GgAAUBxOp1NRUVE3/FUuxI7+/7cvh4aGEjsAAJQzN7oEhQuUAQCA0YgdAABgNGIHAAAYjdgBAABGI3YAAIDRiB0AAGA0YgcAABiN2AEAAEYjdgAAgNGIHQAAYDRiBwAAGI3YAQAARiN2AACA0YgdAABgNGIHAAAYLcDbAwBAeXdifKy3RwB8Uq3kL709giTO7AAAAMMROwAAwGjEDgAAMBqxAwAAjEbsAAAAoxE7AADAaMQOAAAwGrEDAACMRuwAAACjETsAAMBoxA4AADAasQMAAIxG7AAAAKMROwAAwGjEDgAAMBqxAwAAjEbsAAAAoxE7AADAaMQOAAAwGrEDAACMRuwAAACjETsAAMBoxA4AADAasQMAAIxG7AAAAKMROwAAwGjEDgAAMBqxAwAAjEbsAAAAo3k1drZu3arHH39ckZGRstlsWrlypdt2y7KUnJysiIgIBQcHKz4+Xunp6W77nDlzRj169FBoaKgqV66sfv366dy5c2X4TwEAAHyZV2MnNzdXjRs31uzZs6+6ferUqZo5c6ZSU1O1c+dOVaxYUQkJCcrLy3Pt06NHD3311VfasGGDVq9era1bt2rAgAFl9Y8AAAB8nM2yLMvbQ0iSzWbTihUr1LlzZ0k/n9WJjIzUsGHD9PLLL0uScnJy5HA4tGjRInXv3l0HDhxQw4YNtXv3bjVr1kyStHbtWj322GP69ttvFRkZWaz3djqdCgsLU05OjkJDQ2/JPx8Ac50YH+vtEQCfVCv5y1t6/OL+/e2z1+wcO3ZMGRkZio+Pd62FhYWpefPm2r59uyRp+/btqly5sit0JCk+Pl5+fn7auXNnmc8MAAB8T4C3B7iWjIwMSZLD4XBbdzgcrm0ZGRmqUaOG2/aAgABVrVrVtc/V5OfnKz8/3/Xc6XR6amwAAOBjfPbMzq00efJkhYWFuR5RUVHeHgkAANwiPhs74eHhkqTMzEy39czMTNe28PBwZWVluW2/dOmSzpw549rnapKSkpSTk+N6nDx50sPTAwAAX+GzsRMTE6Pw8HClpaW51pxOp3bu3Km4uDhJUlxcnLKzs7V3717XPhs3blRhYaGaN29+zWPb7XaFhoa6PQAAgJm8es3OuXPndOTIEdfzY8eO6bPPPlPVqlVVq1YtDRkyRBMmTFC9evUUExOjMWPGKDIy0nXHVoMGDdS+fXv1799fqampunjxogYNGqTu3bsX+04sAABgNq/Gzp49e9S2bVvX88TERElSr169tGjRIo0YMUK5ubkaMGCAsrOz1apVK61du1ZBQUGu17z99tsaNGiQHnnkEfn5+alLly6aOXNmmf+zAAAA3+Qz37PjTXzPDoDS4Ht2gKvje3YAAADKALEDAACMRuwAAACjETsAAMBoxA4AADAasQMAAIxG7AAAAKMROwAAwGjEDgAAMBqxAwAAjEbsAAAAoxE7AADAaMQOAAAwGrEDAACMRuwAAACjETsAAMBoxA4AADAasQMAAIxG7AAAAKMROwAAwGjEDgAAMBqxAwAAjEbsAAAAoxE7AADAaMQOAAAwGrEDAACMRuwAAACjETsAAMBoxA4AADAasQMAAIxG7AAAAKMROwAAwGjEDgAAMBqxAwAAjEbsAAAAoxE7AADAaMQOAAAwGrEDAACMRuwAAACjETsAAMBoxA4AADAasQMAAIxG7AAAAKMROwAAwGjEDgAAMBqxAwAAjEbsAAAAoxE7AADAaMQOAAAwGrEDAACMRuwAAACjETsAAMBoxA4AADCaT8fO5cuXNWbMGMXExCg4OFh16tTRq6++KsuyXPtYlqXk5GRFREQoODhY8fHxSk9P9+LUAADAl/h07EyZMkVz587VX/7yFx04cEBTpkzR1KlTNWvWLNc+U6dO1cyZM5WamqqdO3eqYsWKSkhIUF5enhcnBwAAviLA2wNcz8cff6wnn3xSHTt2lCTVrl1bf/vb37Rr1y5JP5/VmTFjhkaPHq0nn3xSkvTmm2/K4XBo5cqV6t69u9dmBwAAvsGnz+y0aNFCaWlpOnz4sCTp888/10cffaQOHTpIko4dO6aMjAzFx8e7XhMWFqbmzZtr+/bt1zxufn6+nE6n2wMAAJjJp8/svPLKK3I6napfv778/f11+fJlTZw4UT169JAkZWRkSJIcDofb6xwOh2vb1UyePFnjxo27dYMDAACf4dNndpYuXaq3335b77zzjj755BMtXrxYf/rTn7R48eJSHTcpKUk5OTmux8mTJz00MQAA8DU+fWZn+PDheuWVV1zX3sTGxur48eOaPHmyevXqpfDwcElSZmamIiIiXK/LzMxUkyZNrnlcu90uu91+S2cHAAC+wafP7Jw/f15+fu4j+vv7q7CwUJIUExOj8PBwpaWlubY7nU7t3LlTcXFxZTorAADwTT59Zufxxx/XxIkTVatWLTVq1Eiffvqppk+frr59+0qSbDabhgwZogkTJqhevXqKiYnRmDFjFBkZqc6dO3t3eAAA4BN8OnZmzZqlMWPG6IUXXlBWVpYiIyP1+9//XsnJya59RowYodzcXA0YMEDZ2dlq1aqV1q5dq6CgIC9ODgAAfIXN+uXXEd+mnE6nwsLClJOTo9DQUG+PA6CcOTE+1tsjAD6pVvKXt/T4xf3726ev2QEAACgtYgcAABiN2AEAAEYjdgAAgNGIHQAAYDRiBwAAGI3YAQAARiN2AACA0YgdAABgNGIHAAAYjdgBAABGI3YAAIDRiB0AAGA0YgcAABiN2AEAAEYjdgAAgNGIHQAAYDRiBwAAGI3YAQAARiN2AACA0YgdAABgNGIHAAAYjdgBAABGI3YAAIDRiB0AAGA0YgcAABiN2AEAAEYjdgAAgNGIHQAAYDRiBwAAGI3YAQAARiN2AACA0YgdAABgNGIHAAAYjdgBAABGI3YAAIDRiB0AAGA0YgcAABiN2AEAAEYjdgAAgNGIHQAAYDRiBwAAGI3YAQAARiN2AACA0QK8PcDt4v7hb3p7BMAn7X3td94eAYDhSnRmp127dsrOzi6y7nQ61a5du9LOBAAA4DElip3NmzeroKCgyHpeXp4+/PDDUg8FAADgKTf1MdYXX3zh+t/79+9XRkaG6/nly5e1du1a3XXXXZ6bDgAAoJRuKnaaNGkim80mm8121Y+rgoODNWvWLI8NBwAAUFo3FTvHjh2TZVn6j//4D+3atUvVq1d3bQsMDFSNGjXk7+/v8SEBAABK6qZiJzo6WpJUWFh4S4YBAADwtBLfep6enq5NmzYpKyurSPwkJyeXejAAAABPKNHdWAsWLFCDBg2UnJys5cuXa8WKFa7HypUrPTrgqVOn1LNnT1WrVk3BwcGKjY3Vnj17XNsty1JycrIiIiIUHBys+Ph4paene3QGAABQfpXozM6ECRM0ceJEjRw50tPzuPnpp5/UsmVLtW3bVmvWrFH16tWVnp6uKlWquPaZOnWqZs6cqcWLFysmJkZjxoxRQkKC9u/fr6CgoFs6HwAA8H0lip2ffvpJXbt29fQsRUyZMkVRUVFauHChay0mJsb1vy3L0owZMzR69Gg9+eSTkqQ333xTDodDK1euVPfu3W/5jAAAwLeV6GOsrl27av369Z6epYj3339fzZo1U9euXVWjRg01bdpUCxYscG0/duyYMjIyFB8f71oLCwtT8+bNtX379mseNz8/X06n0+0BAADMVKIzO3Xr1tWYMWO0Y8cOxcbGqkKFCm7bBw8e7JHhjh49qrlz5yoxMVGjRo3S7t27NXjwYAUGBqpXr16uLzV0OBxur3M4HG5fePhrkydP1rhx4zwyIwAA8G0lip358+crJCREW7Zs0ZYtW9y22Ww2j8VOYWGhmjVrpkmTJkmSmjZtqn379ik1NVW9evUq8XGTkpKUmJjoeu50OhUVFVXqeQEAgO8pUewcO3bM03NcVUREhBo2bOi21qBBA/3jH/+QJIWHh0uSMjMzFRER4donMzNTTZo0ueZx7Xa77Ha75wcGAAA+p0TX7JSVli1b6tChQ25rhw8fdn25YUxMjMLDw5WWluba7nQ6tXPnTsXFxZXprAAAwDeV6MxO3759r7v9jTfeKNEwvzZ06FC1aNFCkyZNUrdu3bRr1y7Nnz9f8+fPl/TzR2ZDhgzRhAkTVK9ePdet55GRkercubNHZgAAAOVbiW89/6WLFy9q3759ys7OvuovCC2pBx54QCtWrFBSUpLGjx+vmJgYzZgxQz169HDtM2LECOXm5mrAgAHKzs5Wq1attHbtWr5jBwAASCph7KxYsaLIWmFhoZ5//nnVqVOn1EP9UqdOndSpU6drbrfZbBo/frzGjx/v0fcFAABm8Ng1O35+fkpMTFRKSoqnDgkAAFBqHr1A+euvv9alS5c8eUgAAIBSKdHHWL/8jhrp51/b8N133+mf//xnqb7/BgAAwNNKFDuffvqp23M/Pz9Vr15d06ZNu+GdWgAAAGWpRLGzadMmT88BAABwS5Qodq74/vvvXV/6d88996h69eoeGQoAAMBTSnSBcm5urvr27auIiAg9/PDDevjhhxUZGal+/frp/Pnznp4RAACgxEoUO4mJidqyZYs++OADZWdnKzs7W6tWrdKWLVs0bNgwT88IAABQYiX6GOsf//iHli9frjZt2rjWHnvsMQUHB6tbt26aO3eup+YDAAAolRKd2Tl//rwcDkeR9Ro1avAxFgAA8Cklip24uDiNHTtWeXl5rrULFy5o3Lhx/LZxAADgU0r0MdaMGTPUvn171axZU40bN5Ykff7557Lb7Vq/fr1HBwQAACiNEsVObGys0tPT9fbbb+vgwYOSpGeeeUY9evRQcHCwRwcEAAAojRLFzuTJk+VwONS/f3+39TfeeEPff/+9Ro4c6ZHhAAAASqtE1+zMmzdP9evXL7LeqFEjpaamlnooAAAATylR7GRkZCgiIqLIevXq1fXdd9+VeigAAABPKVHsREVFadu2bUXWt23bpsjIyFIPBQAA4Cklumanf//+GjJkiC5evKh27dpJktLS0jRixAi+QRkAAPiUEsXO8OHD9eOPP+qFF15QQUGBJCkoKEgjR45UUlKSRwcEAAAojRLFjs1m05QpUzRmzBgdOHBAwcHBqlevnux2u6fnAwAAKJUSxc4VISEheuCBBzw1CwAAgMeV6AJlAACA8oLYAQAARiN2AACA0YgdAABgNGIHAAAYjdgBAABGI3YAAIDRiB0AAGA0YgcAABiN2AEAAEYjdgAAgNGIHQAAYDRiBwAAGI3YAQAARiN2AACA0YgdAABgNGIHAAAYjdgBAABGI3YAAIDRiB0AAGA0YgcAABiN2AEAAEYjdgAAgNGIHQAAYDRiBwAAGI3YAQAARiN2AACA0YgdAABgNGIHAAAYjdgBAABGI3YAAIDRiB0AAGC0chU7//u//yubzaYhQ4a41vLy8jRw4EBVq1ZNISEh6tKlizIzM703JAAA8CnlJnZ2796tefPm6b777nNbHzp0qD744AMtW7ZMW7Zs0enTp/XUU095aUoAAOBrykXsnDt3Tj169NCCBQtUpUoV13pOTo5ef/11TZ8+Xe3atdP999+vhQsX6uOPP9aOHTu8ODEAAPAV5SJ2Bg4cqI4dOyo+Pt5tfe/evbp48aLbev369VWrVi1t3779msfLz8+X0+l0ewAAADMFeHuAG3n33Xf1ySefaPfu3UW2ZWRkKDAwUJUrV3ZbdzgcysjIuOYxJ0+erHHjxnl6VAAA4IN8+szOyZMn9dJLL+ntt99WUFCQx46blJSknJwc1+PkyZMeOzYAAPAtPh07e/fuVVZWln7zm98oICBAAQEB2rJli2bOnKmAgAA5HA4VFBQoOzvb7XWZmZkKDw+/5nHtdrtCQ0PdHgAAwEw+/THWI488oi+//NJtrU+fPqpfv75GjhypqKgoVahQQWlpaerSpYsk6dChQzpx4oTi4uK8MTIAAPAxPh07lSpV0r333uu2VrFiRVWrVs213q9fPyUmJqpq1aoKDQ3Viy++qLi4OD344IPeGBkAAPgYn46d4khJSZGfn5+6dOmi/Px8JSQkaM6cOd4eCwAA+IhyFzubN292ex4UFKTZs2dr9uzZ3hkIAAD4NJ++QBkAAKC0iB0AAGA0YgcAABiN2AEAAEYjdgAAgNGIHQAAYDRiBwAAGI3YAQAARiN2AACA0YgdAABgNGIHAAAYjdgBAABGI3YAAIDRiB0AAGA0YgcAABiN2AEAAEYjdgAAgNGIHQAAYDRiBwAAGI3YAQAARiN2AACA0YgdAABgNGIHAAAYjdgBAABGI3YAAIDRiB0AAGA0YgcAABiN2AEAAEYjdgAAgNGIHQAAYDRiBwAAGI3YAQAARiN2AACA0YgdAABgNGIHAAAYjdgBAABGI3YAAIDRiB0AAGA0YgcAABiN2AEAAEYjdgAAgNGIHQAAYDRiBwAAGI3YAQAARiN2AACA0YgdAABgNGIHAAAYjdgBAABGI3YAAIDRiB0AAGA0YgcAABiN2AEAAEbz6diZPHmyHnjgAVWqVEk1atRQ586ddejQIbd98vLyNHDgQFWrVk0hISHq0qWLMjMzvTQxAADwNT4dO1u2bNHAgQO1Y8cObdiwQRcvXtR//dd/KTc317XP0KFD9cEHH2jZsmXasmWLTp8+raeeesqLUwMAAF8S4O0Brmft2rVuzxctWqQaNWpo7969evjhh5WTk6PXX39d77zzjtq1aydJWrhwoRo0aKAdO3bowQcf9MbYAADAh/j0mZ1fy8nJkSRVrVpVkrR3715dvHhR8fHxrn3q16+vWrVqafv27dc8Tn5+vpxOp9sDAACYqdzETmFhoYYMGaKWLVvq3nvvlSRlZGQoMDBQlStXdtvX4XAoIyPjmseaPHmywsLCXI+oqKhbOToAAPCichM7AwcO1L59+/Tuu++W+lhJSUnKyclxPU6ePOmBCQEAgC/y6Wt2rhg0aJBWr16trVu3qmbNmq718PBwFRQUKDs72+3sTmZmpsLDw695PLvdLrvdfitHBgAAPsKnz+xYlqVBgwZpxYoV2rhxo2JiYty233///apQoYLS0tJca4cOHdKJEycUFxdX1uMCAAAf5NNndgYOHKh33nlHq1atUqVKlVzX4YSFhSk4OFhhYWHq16+fEhMTVbVqVYWGhurFF19UXFwcd2IBAABJPh47c+fOlSS1adPGbX3hwoXq3bu3JCklJUV+fn7q0qWL8vPzlZCQoDlz5pTxpAAAwFf5dOxYlnXDfYKCgjR79mzNnj27DCYCAADljU9fswMAAFBaxA4AADAasQMAAIxG7AAAAKMROwAAwGjEDgAAMBqxAwAAjEbsAAAAoxE7AADAaMQOAAAwGrEDAACMRuwAAACjETsAAMBoxA4AADAasQMAAIxG7AAAAKMROwAAwGjEDgAAMBqxAwAAjEbsAAAAoxE7AADAaMQOAAAwGrEDAACMRuwAAACjETsAAMBoxA4AADAasQMAAIxG7AAAAKMROwAAwGjEDgAAMBqxAwAAjEbsAAAAoxE7AADAaMQOAAAwGrEDAACMRuwAAACjETsAAMBoxA4AADAasQMAAIxG7AAAAKMROwAAwGjEDgAAMBqxAwAAjEbsAAAAoxE7AADAaMQOAAAwGrEDAACMRuwAAACjETsAAMBoxA4AADAasQMAAIxG7AAAAKMZEzuzZ89W7dq1FRQUpObNm2vXrl3eHgkAAPgAI2Ln73//uxITEzV27Fh98sknaty4sRISEpSVleXt0QAAgJcZETvTp09X//791adPHzVs2FCpqam644479MYbb3h7NAAA4GXlPnYKCgq0d+9excfHu9b8/PwUHx+v7du3e3EyAADgCwK8PUBp/fDDD7p8+bIcDofbusPh0MGDB6/6mvz8fOXn57ue5+TkSJKcTuctm/Ny/oVbdmygPLuVf+7Kytm8y94eAfBJt/rP95XjW5Z13f3KfeyUxOTJkzVu3Lgi61FRUV6YBri9hc36g7dHAHCrTA4rk7c5e/aswsKu/V7lPnbuvPNO+fv7KzMz0209MzNT4eHhV31NUlKSEhMTXc8LCwt15swZVatWTTab7ZbOC+9zOp2KiorSyZMnFRoa6u1xAHgQf75vL5Zl6ezZs4qMjLzufuU+dgIDA3X//fcrLS1NnTt3lvRzvKSlpWnQoEFXfY3dbpfdbndbq1y58i2eFL4mNDSU/xgChuLP9+3jemd0rij3sSNJiYmJ6tWrl5o1a6b//M//1IwZM5Sbm6s+ffp4ezQAAOBlRsTO008/re+//17JycnKyMhQkyZNtHbt2iIXLQMAgNuPEbEjSYMGDbrmx1bAL9ntdo0dO7bIR5kAyj/+fONqbNaN7tcCAAAox8r9lwoCAABcD7EDAACMRuwAAACjETsAAMBoxA5uC71795bNZpPNZlOFChUUExOjESNGKC8vz9ujASiFK3+2//CHor92ZODAgbLZbOrdu3fZDwafQuzgttG+fXt99913Onr0qFJSUjRv3jyNHTvW22MBKKWoqCi9++67unDh/3/hcl5ent555x3VqlXLi5PBVxA7uG3Y7XaFh4crKipKnTt3Vnx8vDZs2ODtsQCU0m9+8xtFRUXpvffec6299957qlWrlpo2berFyeAriB3clvbt26ePP/5YgYGB3h4FgAf07dtXCxcudD1/4403+JVBcCF2cNtYvXq1QkJCFBQUpNjYWGVlZWn48OHeHguAB/Ts2VMfffSRjh8/ruPHj2vbtm3q2bOnt8eCjzDm10UAN9K2bVvNnTtXubm5SklJUUBAgLp06eLtsQB4QPXq1dWxY0ctWrRIlmWpY8eOuvPOO709FnwEsYPbRsWKFVW3bl1JP5/ibty4sV5//XX169fPy5MB8IS+ffu6fkfi7NmzvTwNfAkfY+G25Ofnp1GjRmn06NFud3AAKL/at2+vgoICXbx4UQkJCd4eBz6E2MFtq2vXrvL39+f/AQKG8Pf314EDB7R//375+/t7exz4EGIHt62AgAANGjRIU6dOVW5urrfHAeABoaGhCg0N9fYY8DE2y7Isbw8BAABwq3BmBwAAGI3YAQAARiN2AACA0YgdAABgNGIHAAAYjdgBAABGI3YAAIDRiB0AAGA0YgeAEU6dOqWePXuqWrVqCg4OVmxsrPbs2ePablmWkpOTFRERoeDgYMXHxys9Pd2LEwMoK8QOgHLvp59+UsuWLVWhQgWtWbNG+/fv17Rp01SlShXXPlOnTtXMmTOVmpqqnTt3qmLFikpISFBeXp4XJwdQFvh1EQDKvVdeeUXbtm3Thx9+eNXtlmUpMjJSw4YN08svvyxJysnJkcPh0KJFi9S9e/eyHBdAGePMDoBy7/3331ezZs3UtWtX1ahRQ02bNtWCBQtc248dO6aMjAzFx8e71sLCwtS8eXNt377dGyMDKEPEDoBy7+jRo5o7d67q1aundevW6fnnn9fgwYO1ePFiSVJGRoYkyeFwuL3O4XC4tgEwV4C3BwCA0iosLFSzZs00adIkSVLTpk21b98+paamqlevXl6eDoC3cWYHQLkXERGhhg0buq01aNBAJ06ckCSFh4dLkjIzM932yczMdG0DYC5iB0C517JlSx06dMht7fDhw4qOjpYkxcTEKDw8XGlpaa7tTqdTO3fuVFxcXJnOCqDs8TEWgHJv6NChatGihSZNmqRu3bpp165dmj9/vubPny9JstlsGjJkiCZMmKB69eopJiZGY8aMUWRkpDp37uzd4QHcctx6DsAIq1evVlJSktLT0xUTE6PExET179/ftd2yLI0dO1bz589Xdna2WrVqpTlz5ujuu+/24tQAygKxAwAAjMY1OwAAwGjEDgAAMBqxAwAAjEbsAAAAoxE7AADAaMQOAAAwGrEDAACMRuwAAACjETsAAMBoxA4AADAasQMAAIxG7AAAAKP9H479gow4S1a/AAAAAElFTkSuQmCC"}, "metadata": {}, "output_type": "display_data"}]}, {"cell_type": "code", "metadata": {"id": "0_0.1228442476004139"}, "execution_count": 5, "source": ["import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\n\n# Define a custom transformer to convert numpy array to PyTorch tensor\nclass ToTensor(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        return torch.tensor(X, dtype=torch.float32)\n\n# Define the model\nclass Classifier(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(Classifier, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n    \n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Define a custom PyTorch estimator for scikit-learn\nclass PyTorchEstimator(BaseEstimator):\n    def __init__(self, input_dim, hidden_dim, output_dim, lr=0.001, epochs=100):\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.lr = lr\n        self.epochs = epochs\n        self.model = None\n    \n    def fit(self, X, y):\n        self.model = Classifier(self.input_dim, self.hidden_dim, self.output_dim)\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n        \n        # Convert target labels to PyTorch tensor\n        y = torch.tensor(y, dtype=torch.long)\n        \n        for epoch in range(self.epochs):\n            optimizer.zero_grad()\n            outputs = self.model(X)\n            loss = criterion(outputs, y)\n            loss.backward()\n            optimizer.step()\n        \n        return self\n    \n    def predict(self, X):\n        with torch.no_grad():\n            outputs = self.model(X)\n            _, predicted = torch.max(outputs, 1)\n            return predicted.numpy()\n\n# Load and preprocess data\n# data = pd.read_csv('sonar_data.csv')  # Replace 'sonar_data.csv' with your dataset file\nX = df.drop('60', axis=1).values\ny = df['60'].map({'R': 0, 'M': 1}).values  # Assuming 'R' and 'M' are the class labels\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the pipeline\nPipeline_1 = Pipeline([\n    ('scaler', StandardScaler()),\n    ('to_tensor', ToTensor()),  # Convert numpy array to PyTorch tensor\n    ('model', PyTorchEstimator(input_dim=X.shape[1], hidden_dim=64, output_dim=2))\n])\n\n# Train the pipeline (including the model)\nPipeline_1.fit(X_train, y_train)"], "outputs": [{"data": {"text/plain": ["Pipeline(steps=[('scaler', StandardScaler()), ('to_tensor', ToTensor()),\n                ('model',\n                 PyTorchEstimator(hidden_dim=64, input_dim=59, output_dim=2))])"], "text/html": ["<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\u25b8\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\u25be\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()), (&#x27;to_tensor&#x27;, ToTensor()),\n                (&#x27;model&#x27;,\n                 PyTorchEstimator(hidden_dim=64, input_dim=59, output_dim=2))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()), (&#x27;to_tensor&#x27;, ToTensor()),\n                (&#x27;model&#x27;,\n                 PyTorchEstimator(hidden_dim=64, input_dim=59, output_dim=2))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ToTensor</label><div class=\"sk-toggleable__content\"><pre>ToTensor()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PyTorchEstimator</label><div class=\"sk-toggleable__content\"><pre>PyTorchEstimator(hidden_dim=64, input_dim=59, output_dim=2)</pre></div></div></div></div></div></div></div>"]}, "metadata": {}, "execution_count": 6, "output_type": "execute_result"}]}, {"cell_type": "code", "metadata": {"id": "5870217729_0.44797861123086813"}, "execution_count": 6, "source": ["X_train.shape"], "outputs": [{"data": {"text/plain": "(140, 59)"}, "metadata": {}, "execution_count": 7, "output_type": "execute_result"}]}, {"cell_type": "code", "metadata": {"id": "0_0.1847387982304265"}, "execution_count": 7, "source": ["Pipeline_1"], "outputs": [{"data": {"text/plain": ["Pipeline(steps=[('scaler', StandardScaler()), ('to_tensor', ToTensor()),\n                ('model',\n                 PyTorchEstimator(hidden_dim=64, input_dim=59, output_dim=2))])"], "text/html": ["<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\u25b8\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\u25be\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()), (&#x27;to_tensor&#x27;, ToTensor()),\n                (&#x27;model&#x27;,\n                 PyTorchEstimator(hidden_dim=64, input_dim=59, output_dim=2))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()), (&#x27;to_tensor&#x27;, ToTensor()),\n                (&#x27;model&#x27;,\n                 PyTorchEstimator(hidden_dim=64, input_dim=59, output_dim=2))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ToTensor</label><div class=\"sk-toggleable__content\"><pre>ToTensor()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PyTorchEstimator</label><div class=\"sk-toggleable__content\"><pre>PyTorchEstimator(hidden_dim=64, input_dim=59, output_dim=2)</pre></div></div></div></div></div></div></div>"]}, "metadata": {}, "execution_count": 8, "output_type": "execute_result"}]}, {"cell_type": "code", "metadata": {"id": "0_0.9000036522612154"}, "execution_count": 8, "source": ["# Make predictions\ny_pred = Pipeline_1.predict(X_test)\ny_pred"], "outputs": [{"data": {"text/plain": "array([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n       1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1])"}, "metadata": {}, "execution_count": 9, "output_type": "execute_result"}]}, {"cell_type": "code", "metadata": {"id": "0_0.02750239435346291"}, "execution_count": 9, "source": ["# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Test Accuracy: {accuracy:.4f}')"], "outputs": [{"name": "stdout", "text": ["Test Accuracy: 0.8571\n"], "output_type": "stream"}]}, {"cell_type": "code", "metadata": {"id": "0_0.7223812758480519"}, "execution_count": 10, "source": ["# Make predictions on new data\nnew_data = np.array([[0.045, 0.099, 0.023, 0.056, 0.078, 0.045, 0.023, 0.045, 0.067, 0.045, 0.023, 0.056, 0.078, 0.045, 0.023, 0.045, 0.067, 0.045, 0.023, 0.056, 0.078, 0.045, 0.023, 0.045, 0.067, 0.045, 0.023, 0.056, 0.078, 0.045, 0.023, 0.045, 0.067, 0.045, 0.023, 0.056, 0.078, 0.045, 0.023, 0.045, 0.067, 0.045, 0.023, 0.056, 0.078, 0.045, 0.023, 0.045, 0.067, 0.045, 0.023, 0.056, 0.078, 0.045, 0.023, 0.045, 0.067, 0.045, 0.023]])\npredicted_class = Pipeline_1.predict(new_data)\nprint(\"Predicted class:\", predicted_class)"], "outputs": [{"name": "stdout", "text": ["Predicted class: [1]\n"], "output_type": "stream"}]}, {"cell_type": "code", "metadata": {"id": "0_0.9026383650786192"}, "execution_count": 11, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\nsaved_model = nb.save_model(model = Pipeline_1, modelName = 'Clf_Torch_V7', modelType = 'ml', X = None, y = None, estimator_type='')\n#X and y are training datasets to get explainer dashboard.\n#estimator_type is to specify algorithm type i.e., classification and regression.\n#Only 'ml\u2019 models with tabular data as input will support in Explainer Dashboard.\n#Choose modelType = 'ml' for machine learning models, modelType = 'cv' for computer vision models and modelType = 'dp' for data transformation pickle files. \n#Provide \u2018column_headers\u2019 as a parameter if they have to be saved in the model.\n#If using custom layer in keras, use native save functionality from keras."], "outputs": [{"name": "stdout", "text": ["WARN: Training data is not provided. Unable to generate Explainer Dashboard\n"], "output_type": "stream"}]}, {"cell_type": "code", "metadata": {"id": "5870217729_0.6241652295171205"}, "execution_count": null, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\nloaded_model = nb.load_saved_model('11561714646713949')"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "5870217729_0.09886819691416604"}, "execution_count": null, "source": ["nb.predict(model = loaded_model, dataframe = X_test, modeltype='ml') \n #Choose modeltype 'ml' for machine learning models and 'cv' for computer vision model \n #ex: For machine learning model nb.predict(model = model, modeltype = 'ml', dataframe = df) \n #ex: For computer vision keras model nb.predict(model = model, modeltype = 'cv', imgs = imgs, imgsize = (28, 28), dim = 1, class_names = class_names) \n #and for pytorch model(model = model, modeltype = 'cv', imgs = imgs, class_names = class_names) \n #Note: incase any error in prediction user squeezed image data in keras"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "5870217729_0.3847496819176508"}, "execution_count": null, "source": ["import logging "], "outputs": []}, {"cell_type": "code", "metadata": {"id": "5870217729_0.029709653101915423"}, "execution_count": null, "source": ["logging.basicConfig(level=logging.INFO)"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "5870217729_0.5765715041385542"}, "execution_count": null, "source": ["logging.info(\"%%%%\"*100)"], "outputs": []}], "metadata": {}, "nbformat": 4, "nbformat_minor": 2}