{"cells": [{"cell_type": "markdown", "metadata": {"id": "0_0.10825913572213075"}, "execution_count": null, "source": ["<h1>Deep Neural Networks for MNIST Classification</h1>"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.1733003818653731"}, "execution_count": null, "source": ["Load Tensorflow"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.6563262492115016"}, "execution_count": null, "source": ["#Import Tensorflow\nimport tensorflow as tf\ntf.random.set_seed(42)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.23691686661778788"}, "execution_count": null, "source": ["Collect Data"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.9995100998744757"}, "execution_count": null, "source": ["We will use MNIST dataset for this exercise. This dataset contains images of hand written numbers with each image being a black & white picture of size 28x28. We will download the data using tensorflow API. The dataset has 60,000 training examples and 10,000 test examples. Please note that images have already been converted to numpy arrays.\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.6732807676984363"}, "execution_count": null, "source": ["#Download dataset\n(trainX, trainY),(testX, testY) = tf.keras.datasets.mnist.load_data()"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.9810202907097554"}, "execution_count": null, "source": ["#Check number of training examples and size of each example\ntrainX.shape"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.6070757862498763"}, "execution_count": null, "source": ["testX.shape"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.6742770433487473"}, "execution_count": null, "source": ["Visualize the data"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.07105876774000985"}, "execution_count": null, "source": ["import matplotlib.pyplot as plt\nimport numpy as np"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.08792920791776604"}, "execution_count": null, "source": ["#function to display test image\ndef show_random_test_image():\n  #Get a random integer between 0 and number of test examples\n  img_num = np.random.randint(0, testX.shape[0])\n  #SHOW THE IMAGE from test dataset\n  plt.imshow(testX[img_num],cmap='gray')\n  plt.suptitle('Number: '+ str(testY[img_num]))\n  plt.show()"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.12364567587305997"}, "execution_count": null, "source": ["#Run the function multiple times to look at different test images\nshow_random_test_image()"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.16890229382366106"}, "execution_count": null, "source": ["<h1>Convert Output label to probabilities"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.07432760652801895"}, "execution_count": null, "source": ["As our model will predict 10 probabilities (probability of 0, 1, 2...9) for each image, we need to have 10 actual probabilities (probabilities of 0,1,2,...9). Here we convert our single number label to 10 actual probabilities using One hot encoding. In one hot encoding, a number gets represented by multiple numbers e.g. number_classes (10 in this example). Out of 10 numbers in One hot encoding, 9 will have value of '0' whereas number will have value of 1."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.6687437980596882"}, "execution_count": null, "source": ["#Now check the label after conversion to 10 numbers\ntestY[0]"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.1500402234318896"}, "execution_count": null, "source": ["#Convert single number labels to 10 numbers using One hot encoding. In Keras API, \n#we can use 'to_categorical' method to do the same. We are converting both test \n#and training labels.\n\ntrainY= tf.keras.utils.to_categorical(trainY, num_classes= 10)\ntestY = tf.keras.utils.to_categorical(testY, num_classes= 10)"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.21621593554011853"}, "execution_count": null, "source": ["testY[0]"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.863066888299949"}, "execution_count": null, "source": ["For our first test image (which has 7 as indicated above), we now have 10 labels or 10 probabilities. First number is probability for picture having number '0' in it, 2nd number is probability if picture having number '1' in it and so on. Here all the probabilities are 0% except for probability being 100% for number 7."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.5136852749429568"}, "execution_count": null, "source": ["<h1>Build the Model(Graph)</h1>"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.012409253214147542"}, "execution_count": null, "source": ["#Initialize the Sequential Model\nmodel = tf.keras.models.Sequential()"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.5859212974665984"}, "execution_count": null, "source": ["**Reshape data** from 2D to 1D -> 28x28 to 784. This is needed as Dense layer requires each example to be 1D i.e a Vector. Also note that our input data shape is (28,28) for MNIST."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.590183139672759"}, "execution_count": null, "source": ["model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.6712228847855795"}, "execution_count": null, "source": ["**Normalize the data** : From now on, we will add a layer to normalize our data inside the model (as normalization is also a math function). We can use BatchNormalization layer to do the same. This means, we need not save our normalizer object using pickle separately."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.1631891825339209"}, "execution_count": null, "source": ["model.add(tf.keras.layers.BatchNormalization())"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.04780072634353427"}, "execution_count": null, "source": ["**Add hidden layers**: We will build a model with 4 hidden layers. Number of neurons in hidden layer will be 200, 100, 60 and 30 respectively. Both number of hidden layers and number of neurons in each hidden layer are hyperparameters i.e you can change these values to improve the model. Output of each neuron in hidden layer will be passed through an activation function."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.33995538344093146"}, "execution_count": null, "source": ["#Add 1st Hidden layer\nmodel.add(tf.keras.layers.Dense(200, activation='sigmoid'))"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.47460748360792726"}, "execution_count": null, "source": ["#Add 2nd Hidden Layer\nmodel.add(tf.keras.layers.Dense(100, activation='sigmoid'))"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.03129388693611035"}, "execution_count": null, "source": ["#Add 3rd Hidden Layer\nmodel.add(tf.keras.layers.Dense(60, activation='sigmoid'))"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.30670502780595243"}, "execution_count": null, "source": ["#Add 4th Hidden Layer\nmodel.add(tf.keras.layers.Dense(30, activation='sigmoid'))"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.537576891060503"}, "execution_count": null, "source": ["**Add Output layer**: Dense Layer to create **10 equations** which provides 10 outputs after applying softmax."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.1008133733778318"}, "execution_count": null, "source": ["model.add(tf.keras.layers.Dense(10, activation='softmax'))"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.8421604967585985"}, "execution_count": null, "source": ["**Compile** the model. We will use non-default learning rate (which is usually set to 0.01 in Keras) for our model. So first we will create an optimizer object and specify the learning rate.\n\n\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.27565974406200566"}, "execution_count": null, "source": ["#Create optimizer with learning rate of 0.03\nsgd_optimizer = tf.keras.optimizers.SGD(learning_rate=0.03)"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.4324707983475444"}, "execution_count": null, "source": ["from keras.losses import categorical_crossentropy\nmodel.compile(optimizer = sgd_optimizer,\n              loss= categorical_crossentropy,\n              metrics=['accuracy'])"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.08794084128131341"}, "execution_count": null, "source": ["Train the Model"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.520723222513058"}, "execution_count": null, "source": ["model.fit(trainX,trainY, #Training data - Features and One hot encoded labels         \n          validation_data=(testX,testY), #Test data\n          epochs=50, #Number of iterations\n          batch_size= 32) #Here we train model with 32 examples at a time. You can change this number to see if model accuracy improves."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.11779565267128578"}, "execution_count": null, "source": ["Save the Model"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.31339154174911776"}, "execution_count": null, "source": ["#Save the model in current directory\nmodel.save('mnist_dnn_v1.h5')"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.39673008562667"}, "execution_count": null, "source": ["## Model Prediction"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.16339112654973453"}, "execution_count": null, "source": ["Prediction on Test Image"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.9035422406354248"}, "execution_count": null, "source": ["It tells us model can take any number of examples as in put ('None' in shape) and each example should have 28x28 shape (2D)."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.17619906529135565"}, "execution_count": null, "source": ["#Shape of each example in test dataset\ntestX[0].shape"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.4801629412658799"}, "execution_count": null, "source": ["#Make it 3 dimension shape i.e make it (1,28,28). This will mean one example with that example having a shape of 28x28\ninput_data = np.expand_dims(testX[0], axis=0)\ninput_data.shape"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.2057823405459196"}, "execution_count": null, "source": ["Model Prediction"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.8115454323206388"}, "execution_count": null, "source": ["#Model prediction\npred = model.predict(input_data)\npred"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.9648556767204066"}, "execution_count": null, "source": ["#Model prediction shape\npred.shape"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.17062819909176152"}, "execution_count": null, "source": ["#Model prediction for first example\npred[0]"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.6257391556257275"}, "execution_count": null, "source": ["Find the number for which probability is highest using 'argmax' function"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.996403510002351"}, "execution_count": null, "source": ["#This gives us predicted label\nnp.argmax(pred[0])"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.3370224281153018"}, "execution_count": null, "source": ["#Actual label\nnp.argmax(testY[0])"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.9247938720150783"}, "execution_count": null, "source": ["#Lets visualize the image as well\nimport matplotlib.pyplot as plt\nplt.imshow(testX[0],cmap='gray')\nplt.show()"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.5398761261264988"}, "execution_count": null, "source": ["As we see, Deep Learning model can achieve **much better results** compared to Logistic Regression we have used earlier. Deep Learning allows our model to learn **better features** (called hidden features) and hence usually provide better results (Please note that this may not always be true for all datasets).\n\nSome **questions** to ponder...\n1. Why did training start slow i.e after 1st iteration the validation accuracy was <20%.\n2. By the end of the training, the training accuracy is higher than test accuracy. Is that ok?"], "outputs": []}], "metadata": {}, "nbformat": 4, "nbformat_minor": 2}