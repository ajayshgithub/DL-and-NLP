{"cells": [{"cell_type": "markdown", "metadata": {"id": "0_0.1995233599879398"}, "execution_count": null, "source": ["> ### RIIAA 2.0 \u2013 Workshop \n> **Deep Learning as a Service** <br>\n> **Instructor:** [Rodolfo Ferro](https://rodolfoferro.xyz) <br>\n> **Email:** <ferro@cimat.mx> <br>\n> **Twitter:** <https://twitter.com/FerroRodolfo/> <br>\n> **GitHub:** <https://github.com/RodolfoFerro/> <br>"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.3975294028428036"}, "execution_count": null, "source": ["# Iris Classification Problem\n\nAlong this notebook we'll explain how to use the power of cloud computing with Google Colab for a classical example\u00a0\u2013*The Iris Classification Problem*\u2013 using the popular [Iris flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set).\n\nFor this classification problem we will build a simple feed-forward full-connected artificial neural network.\n\nThe Python framework that we will be using is [Tensorflow 2.0](https://www.tensorflow.org) with the [Keras](https://keras.io/) module.\n\n\n### Problem statement\n\nBefore we tackle the problem an ANN, let's understand what we'll be doing: \n\n* If we feed our neural network with Iris data, the model should be able to determine what species it is.\n\n> #### What do we need to do?\n> Train a _Deep Learning_ model (in this case) using a known dataset: [Iris flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set).\n>\n> Specifically, we are going to do the following:\n> - Load the dataset\n> - Preprocess the data\n> - Build the model\n> - Set hyperparameters \n> - Train the model\n> - Save and download the trained model\n> - Predict data\n\n## Installing dependencies\n\nFor our training we will be using Tensorflow 2.0, so we want to be sure it is installed on its latest version:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.8185147677387357"}, "execution_count": null, "source": ["# Let's install Tensorflow 2.0:\n!!pip install -q tensorflow==2.0.0-rc0\n\n# And verify that it is now in its latest version:\nimport tensorflow as tf\nprint(tf.__version__)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.7849873885186816"}, "execution_count": null, "source": ["## The Iris dataset"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.5845856130357161"}, "execution_count": null, "source": ["from IPython.display import HTML\nurl = 'https://en.wikipedia.org/wiki/Iris_flower_data_set'\niframe = '<iframe src=' + url + ' width=\"100%\" height=400></iframe>'\nHTML(iframe)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.5101832001400539"}, "execution_count": null, "source": ["## Importing the dataset"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.0447013509942531"}, "execution_count": null, "source": ["# Importing dataset from scikit-learn and other useful packages:\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# We will fix a random seed for reproducibility:\nseed = 11\nnp.random.seed(seed)"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "5259389462_0.6599206180505159"}, "execution_count": null, "source": ["# We now import the Iris dataset:\niris = load_iris()\n\nx = iris.data \ny = iris.target  \nnames = iris.target_names  \nfeature_names = iris.feature_names "], "outputs": []}, {"cell_type": "code", "metadata": {"id": "5259389462_0.9085697834112156"}, "execution_count": null, "source": ["x"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.09501642813923405"}, "execution_count": null, "source": ["# And set the features and labels vectors from it:\n# Set x as 'data' from iris\n# Set y as 'target' from iris\n# Set names as 'target_names' from iris\n# Set feature_names as 'feature_names' from iris\n\n# We can load some elements to verify the contents in the dataset:\nelements_to_display = [0, 10, 20, 40, 60]\nfor element in elements_to_display:\n    print(f\"Element {element}th:\")\n    print(f\"  - Features: {x[element]}\")\n    print(f\"  - Target: {y[element]}\")\n    print(f\"  - Species: {names[element % 3]}\")\n    print()"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "5259389462_0.11917435266494758"}, "execution_count": null, "source": ["names = iris.target_names  \nfeature_names = iris.feature_names "], "outputs": []}, {"cell_type": "code", "metadata": {"id": "5259389462_0.6395842756499854"}, "execution_count": null, "source": ["names"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "5259389462_0.9985375714487104"}, "execution_count": null, "source": ["feature_names"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.5880442288060657"}, "execution_count": null, "source": ["## Preprocess dataset\n\nThe preprocess step results very important in many cases. For this case, we will just need to do a very simple transformation: a one hot encode process."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.4672528651584966"}, "execution_count": null, "source": ["## Let's talk about the model...\n\nWe will be using a very simple model, a feed-forward multi-layer perceptron.\n\n### Let's create the model with Keras!\n\nFirst of all, let's import what we'll use:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.8486372607924193"}, "execution_count": null, "source": ["from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\nimport joblib\n\n# Load the Iris dataset\niris = load_iris()\nx = iris.data\ny = iris.target\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.5076892595572218"}, "execution_count": null, "source": ["# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=seed)\n\n# Define the pipeline\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', MLPClassifier(hidden_layer_sizes=(12, 24, 36), activation='relu', max_iter=1000))  # Example model\n])\n\n# Fit the pipeline to the training data\npipeline.fit(x_train, y_train)"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.8112618434703454"}, "execution_count": null, "source": ["# Save the pipeline as a file\njoblib.dump(pipeline, 'iris_pipeline.pkl')"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.19681280675878954"}, "execution_count": null, "source": ["# Load the pipeline from the file\nloaded_pipeline = joblib.load('iris_pipeline.pkl')"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.27358826797915214"}, "execution_count": null, "source": ["# Use the loaded pipeline for predictions\npredictions = loaded_pipeline.predict(x_test)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "5259389462_0.4697285628040111"}, "execution_count": null, "source": ["Species:\n'setosa' --> 0\n'versicolor' --> 1 \n'virginica' --> 2"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.6860625575250345"}, "execution_count": null, "source": ["predictions"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.6153066699278242"}, "execution_count": null, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\nsaved_model = nb.save_model(model = pipeline, modelName = 'tf_cls', modelType = 'ml', X = None, y = None, estimator_type='')\n#X and y are training datasets to get explainer dashboard.\n#estimator_type is to specify algorithm type i.e., classification and regression.\n#Only 'ml\u2019 models with tabular data as input will support in Explainer Dashboard.\n#Choose modelType = 'ml' for machine learning models, modelType = 'cv' for computer vision models and modelType = 'dp' for data transformation pickle files. \n#Provide \u2018column_headers\u2019 as a parameter if they have to be saved in the model.\n#If using custom layer in keras, use native save functionality from keras."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.9338571863266751"}, "execution_count": null, "source": ["### Useful resources\n\n- Sequential model: <https://keras.io/getting-started/sequential-model-guide/>\n- Classifying the Iris Data Set with Keras: <https://janakiev.com/notebooks/keras-iris/>\n\n### Building the model"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.7250123969564561"}, "execution_count": null, "source": ["### Training the model\n\nIn order to train the model, we first need to set its training hyperparameters."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.6746921512305923"}, "execution_count": null, "source": ["### Evaluating the results"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.00015879662638651482"}, "execution_count": null, "source": ["### Plot the training along the time"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.1325001182724932"}, "execution_count": null, "source": ["def plot_loss(history):\n    plt.style.use(\"ggplot\")\n    plt.figure(figsize=(8, 4))\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title(\"Model's training loss\")\n    plt.xlabel(\"Epoch #\")\n    plt.ylabel(\"Loss\")\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n\ndef plot_accuracy(history):\n    plt.style.use(\"ggplot\")\n    plt.figure(figsize=(8, 4))\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title(\"Model's training accuracy\")\n    plt.xlabel(\"Epoch #\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.5600938417550236"}, "execution_count": null, "source": ["plot_loss(history)\nplot_accuracy(history)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.8558126043628234"}, "execution_count": null, "source": ["_How can we save these plots?_\n\n## Saving a model\n\nTo save the trained model we will basically do two things:\n\n1. Serialize the model into a JSON file, which will save the architecture of our model.\n2. Serialize the weights into a HDF5 file, which will save all parameters of our model."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.9755051945364306"}, "execution_count": null, "source": ["# Serialize model to JSON:\nmodel_json = model.to_json()\nwith open(\"iris_model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n\n# Serialize weights to HDF5 (h5py needed):\nmodel.save_weights(\"iris_model.h5\")\nprint(\"Model saved to disk.\")"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.583835806629279"}, "execution_count": null, "source": ["## Loading a trained model\nWe will basically do three things:\n\n1. Load the model from a JSON file.\n2. Load the weights from a HDF5 file.\n3. (Re)Compile the trained model."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.15713325373715015"}, "execution_count": null, "source": ["# Load json and create model:\nfrom tensorflow.keras.models import model_from_json\n\njson_file = open('iris_model.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n\n# Load weights into loaded model:\nloaded_model.load_weights(\"iris_model.h5\")\nprint(\"Model loaded from disk.\")"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.8794954253415024"}, "execution_count": null, "source": ["# Evaluate loaded model on test data:\nloaded_model.compile(loss='categorical_crossentropy',\n                     optimizer='adam',\n                     metrics=['accuracy'])\n\nscore = loaded_model.evaluate(x_test, y_test, verbose=1)\nprint(f'Test accuracy: {score[1]}')"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.09622059098719493"}, "execution_count": null, "source": ["## Predicting from new data\n\nNow that we have a trained model, how do we use it?\n\nIt is as simple as follows:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.15517543174537973"}, "execution_count": null, "source": ["# Remembering some elements:\nfor element in elements_to_display:\n    prediction_vector = model.predict(np.array([x[element]]))\n    print(f\"Element {element}th:\")\n    print(f\"  - Features: {x[element]}\")\n    print(f\"  - Target: {y[element]}\")\n    print(f\"  - Species: {names[np.argmax(y[element])]}\")\n    print(f\"  - Predicted species: {names[np.argmax(prediction_vector)]}\")\n    print()"], "outputs": []}], "metadata": {}, "nbformat": 4, "nbformat_minor": 2}