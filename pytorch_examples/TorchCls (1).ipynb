{"cells": [{"cell_type": "code", "metadata": {"id": "0_0.1630453579344151"}, "execution_count": 1, "source": ["!!pip install torch skorch scikit-learn"], "outputs": [{"data": {"text/plain": "['Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.1)',\n 'Collecting skorch',\n '  Obtaining dependency information for skorch from https://files.pythonhosted.org/packages/7a/7e/aca63fab758a7ddb1f043a0ee8c77f06bbb92d99bb8baf17047f09a328f5/skorch-0.15.0-py3-none-any.whl.metadata',\n '  Downloading skorch-0.15.0-py3-none-any.whl.metadata (11 kB)',\n 'Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.3.0)',\n 'Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.3)',\n 'Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.11.0)',\n 'Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)',\n 'Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)',\n 'Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.0.3)',\n 'Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch) (11.7.99)',\n 'Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch) (11.7.99)',\n 'Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/lib/python3.10/site-packages (from torch) (11.7.101)',\n 'Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch) (8.5.0.96)',\n 'Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch) (11.10.3.66)',\n 'Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.10/site-packages (from torch) (10.9.0.58)',\n 'Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/lib/python3.10/site-packages (from torch) (10.2.10.91)',\n 'Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/lib/python3.10/site-packages (from torch) (11.4.0.1)',\n 'Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/lib/python3.10/site-packages (from torch) (11.7.4.91)',\n 'Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/lib/python3.10/site-packages (from torch) (2.14.3)',\n 'Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/lib/python3.10/site-packages (from torch) (11.7.91)',\n 'Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch) (2.0.0)',\n 'Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (68.1.2)',\n 'Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)',\n 'Requirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch) (3.29.3)',\n 'Requirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch) (18.1.4)',\n 'Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.10/site-packages (from skorch) (1.23.5)',\n 'Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from skorch) (1.11.2)',\n 'Requirement already satisfied: tabulate>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from skorch) (0.9.0)',\n 'Requirement already satisfied: tqdm>=4.14.0 in /opt/conda/lib/python3.10/site-packages (from skorch) (4.65.0)',\n 'Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.3.2)',\n 'Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)',\n 'Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)',\n 'Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)',\n 'Downloading skorch-0.15.0-py3-none-any.whl (239 kB)',\n '\\x1b[?25l   \\x1b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\\x1b[0m \\x1b[32m0.0/239.3 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n '\\x1b[2K   \\x1b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\\x1b[0m \\x1b[32m239.3/239.3 kB\\x1b[0m \\x1b[31m14.4 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n '\\x1b[?25hInstalling collected packages: skorch',\n 'Successfully installed skorch-0.15.0',\n '',\n '\\x1b[1m[\\x1b[0m\\x1b[34;49mnotice\\x1b[0m\\x1b[1;39;49m]\\x1b[0m\\x1b[39;49m A new release of pip is available: \\x1b[0m\\x1b[31;49m23.2.1\\x1b[0m\\x1b[39;49m -> \\x1b[0m\\x1b[32;49m24.0\\x1b[0m',\n '\\x1b[1m[\\x1b[0m\\x1b[34;49mnotice\\x1b[0m\\x1b[1;39;49m]\\x1b[0m\\x1b[39;49m To update, run: \\x1b[0m\\x1b[32;49mpip install --upgrade pip\\x1b[0m']"}, "metadata": {}, "execution_count": 2, "output_type": "execute_result"}]}, {"cell_type": "code", "metadata": {"id": "5256759304_0.7039096822496349"}, "execution_count": 2, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\ndata = nb.get_data('11561714547878799', '@SYS.USERID', 'True', {}, [])\n\ndata\n# The first function parameter refers to the service ID of the dataset.\n# @SYS.USERID refers to the user ID of the current user.\n# If the Sandbox key is 'false', it is referred to as a dataset, and if it's 'true', then the file is a sandbox file.\n# {} refers to the filters applied to the dataset.\n# [] refers to the data preparations applied to the dataset.\n# After [], users can specify the number of rows to limit the headcount of the dataset with a comma separator."], "outputs": [{"data": {"text/plain": ["             0       1       2       3       4       5       6       7       8       9      10      11      12      13      14      15      16      17      18      19      20      21      22      23      24      25      26      27      28      29      30      31      32      33      34      35      36      37      38      39      40      41      42      43      44      45      46      47      48      49      50      51      52      53      54      56      57      58      59 Target\n0.0442  0.0477  0.0049  0.0581  0.0278  0.0678  0.1664  0.1490  0.0974  0.1268  0.1109  0.2375  0.2007  0.2140  0.1109  0.2036  0.2468  0.6682  0.8345  0.8252  0.8017  0.8982  0.9664  0.8515  0.6626  0.3241  0.2054  0.5669  0.5726  0.4877  0.7532  0.7600  0.5185  0.4120  0.5560  0.5569  0.1336  0.3831  0.4611  0.4330  0.2556  0.1466  0.3489  0.2659  0.0944  0.1370  0.1344  0.0416  0.0719  0.0637  0.0210  0.0204  0.0216  0.0135  0.0055  0.0073  0.0080  0.0105  0.0059  0.0105      R\n0.0311  0.0491  0.0692  0.0831  0.0079  0.0200  0.0981  0.1016  0.2025  0.0767  0.1767  0.2555  0.2812  0.2722  0.3227  0.3463  0.5395  0.7911  0.9064  0.8701  0.7672  0.2957  0.4148  0.6043  0.3178  0.3482  0.6158  0.8049  0.6289  0.4999  0.5830  0.6660  0.4124  0.1260  0.2487  0.4676  0.5382  0.3150  0.2139  0.1848  0.1679  0.2328  0.1015  0.0713  0.0615  0.0779  0.0761  0.0845  0.0592  0.0068  0.0089  0.0087  0.0032  0.0130  0.0188  0.0101  0.0229  0.0182  0.0046  0.0038      R\n0.0206  0.0132  0.0533  0.0569  0.0647  0.1432  0.1344  0.2041  0.1571  0.1573  0.2327  0.1785  0.1507  0.1916  0.2061  0.2307  0.2360  0.1299  0.3812  0.5858  0.4497  0.4876  1.0000  0.8675  0.4718  0.5341  0.6197  0.7143  0.5605  0.3728  0.2481  0.1921  0.1386  0.3325  0.2883  0.3228  0.2607  0.2040  0.2396  0.1319  0.0683  0.0334  0.0716  0.0976  0.0787  0.0522  0.0500  0.0231  0.0221  0.0144  0.0307  0.0386  0.0147  0.0018  0.0100  0.0096  0.0077  0.0180  0.0109  0.0070      R\n0.0094  0.0166  0.0398  0.0359  0.0681  0.0706  0.1020  0.0893  0.0381  0.1328  0.1303  0.0273  0.0644  0.0712  0.1204  0.0717  0.1224  0.2349  0.3684  0.3918  0.4925  0.8793  0.9606  0.8786  0.6905  0.6937  0.5674  0.6540  0.7802  0.7575  0.5836  0.6316  0.8108  0.9039  0.8647  0.6695  0.4027  0.2370  0.2685  0.3662  0.3267  0.2200  0.2996  0.2205  0.1163  0.0635  0.0465  0.0422  0.0174  0.0172  0.0134  0.0141  0.0191  0.0145  0.0065  0.0129  0.0217  0.0087  0.0077  0.0122      R\n0.0333  0.0221  0.0270  0.0481  0.0679  0.0981  0.0843  0.1172  0.0759  0.0920  0.1475  0.0522  0.1119  0.0970  0.1174  0.1678  0.1642  0.1205  0.0494  0.1544  0.3485  0.6146  0.9146  0.9364  0.8677  0.8772  0.8553  0.8833  1.0000  0.8296  0.6601  0.5499  0.5716  0.6859  0.6825  0.5142  0.2750  0.1358  0.1551  0.2646  0.1994  0.1883  0.2746  0.1651  0.0575  0.0695  0.0598  0.0456  0.0021  0.0068  0.0036  0.0022  0.0032  0.0060  0.0054  0.0063  0.0143  0.0132  0.0051  0.0041      R\n...        ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...    ...\n0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328  0.2684  0.3108  0.2933  0.2275  0.0994  0.1801  0.2200  0.2732  0.2862  0.2034  0.1740  0.4130  0.6879  0.8120  0.8453  0.8919  0.9300  0.9987  1.0000  0.8104  0.6199  0.6041  0.5547  0.4160  0.1472  0.0849  0.0608  0.0969  0.1411  0.1676  0.1200  0.1201  0.1036  0.1977  0.1339  0.0902  0.1085  0.1521  0.1363  0.0858  0.0290  0.0203  0.0116  0.0098  0.0199  0.0033  0.0101  0.0065  0.0115  0.0193  0.0157      M\n0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  0.1018  0.1030  0.2154  0.3085  0.3425  0.2990  0.1402  0.1235  0.1534  0.1901  0.2429  0.2120  0.2395  0.3272  0.5949  0.8302  0.9045  0.9888  0.9912  0.9448  1.0000  0.9092  0.7412  0.7691  0.7117  0.5304  0.2131  0.0928  0.1297  0.1159  0.1226  0.1768  0.0345  0.1562  0.0824  0.1149  0.1694  0.0954  0.0080  0.0790  0.1255  0.0647  0.0179  0.0051  0.0061  0.0093  0.0135  0.0063  0.0063  0.0034  0.0032  0.0062  0.0067      M\n0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258  0.2529  0.2716  0.2374  0.1878  0.0983  0.0683  0.1503  0.1723  0.2339  0.1962  0.1395  0.3164  0.5888  0.7631  0.8473  0.9424  0.9986  0.9699  1.0000  0.8630  0.6979  0.7717  0.7305  0.5197  0.1786  0.1098  0.1446  0.1066  0.1440  0.1929  0.0325  0.1490  0.0328  0.0537  0.1309  0.0910  0.0757  0.1059  0.1005  0.0535  0.0235  0.0155  0.0160  0.0029  0.0051  0.0062  0.0089  0.0140  0.0138  0.0077  0.0031      M\n0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945  0.2354  0.2898  0.2812  0.1578  0.0273  0.0673  0.1444  0.2070  0.2645  0.2828  0.4293  0.5685  0.6990  0.7246  0.7622  0.9242  1.0000  0.9979  0.8297  0.7032  0.7141  0.6893  0.4961  0.2584  0.0969  0.0776  0.0364  0.1572  0.1823  0.1349  0.0849  0.0492  0.1367  0.1552  0.1548  0.1319  0.0985  0.1258  0.0954  0.0489  0.0241  0.0042  0.0086  0.0046  0.0126  0.0036  0.0035  0.0034  0.0079  0.0036  0.0048      M\n0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843  0.2354  0.2720  0.2442  0.1665  0.0336  0.1302  0.1708  0.2177  0.3175  0.3714  0.4552  0.5700  0.7397  0.8062  0.8837  0.9432  1.0000  0.9375  0.7603  0.7123  0.8358  0.7622  0.4567  0.1715  0.1549  0.1641  0.1869  0.2655  0.1713  0.0959  0.0768  0.0847  0.2076  0.2505  0.1862  0.1439  0.1470  0.0991  0.0041  0.0154  0.0116  0.0181  0.0146  0.0129  0.0047  0.0039  0.0061  0.0040  0.0036  0.0061  0.0115      M\n\n[175 rows x 60 columns]"], "text/html": ["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>34</th>\n      <th>35</th>\n      <th>36</th>\n      <th>37</th>\n      <th>38</th>\n      <th>39</th>\n      <th>40</th>\n      <th>41</th>\n      <th>42</th>\n      <th>43</th>\n      <th>44</th>\n      <th>45</th>\n      <th>46</th>\n      <th>47</th>\n      <th>48</th>\n      <th>49</th>\n      <th>50</th>\n      <th>51</th>\n      <th>52</th>\n      <th>53</th>\n      <th>54</th>\n      <th>56</th>\n      <th>57</th>\n      <th>58</th>\n      <th>59</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0.0442</th>\n      <td>0.0477</td>\n      <td>0.0049</td>\n      <td>0.0581</td>\n      <td>0.0278</td>\n      <td>0.0678</td>\n      <td>0.1664</td>\n      <td>0.1490</td>\n      <td>0.0974</td>\n      <td>0.1268</td>\n      <td>0.1109</td>\n      <td>0.2375</td>\n      <td>0.2007</td>\n      <td>0.2140</td>\n      <td>0.1109</td>\n      <td>0.2036</td>\n      <td>0.2468</td>\n      <td>0.6682</td>\n      <td>0.8345</td>\n      <td>0.8252</td>\n      <td>0.8017</td>\n      <td>0.8982</td>\n      <td>0.9664</td>\n      <td>0.8515</td>\n      <td>0.6626</td>\n      <td>0.3241</td>\n      <td>0.2054</td>\n      <td>0.5669</td>\n      <td>0.5726</td>\n      <td>0.4877</td>\n      <td>0.7532</td>\n      <td>0.7600</td>\n      <td>0.5185</td>\n      <td>0.4120</td>\n      <td>0.5560</td>\n      <td>0.5569</td>\n      <td>0.1336</td>\n      <td>0.3831</td>\n      <td>0.4611</td>\n      <td>0.4330</td>\n      <td>0.2556</td>\n      <td>0.1466</td>\n      <td>0.3489</td>\n      <td>0.2659</td>\n      <td>0.0944</td>\n      <td>0.1370</td>\n      <td>0.1344</td>\n      <td>0.0416</td>\n      <td>0.0719</td>\n      <td>0.0637</td>\n      <td>0.0210</td>\n      <td>0.0204</td>\n      <td>0.0216</td>\n      <td>0.0135</td>\n      <td>0.0055</td>\n      <td>0.0073</td>\n      <td>0.0080</td>\n      <td>0.0105</td>\n      <td>0.0059</td>\n      <td>0.0105</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>0.0311</th>\n      <td>0.0491</td>\n      <td>0.0692</td>\n      <td>0.0831</td>\n      <td>0.0079</td>\n      <td>0.0200</td>\n      <td>0.0981</td>\n      <td>0.1016</td>\n      <td>0.2025</td>\n      <td>0.0767</td>\n      <td>0.1767</td>\n      <td>0.2555</td>\n      <td>0.2812</td>\n      <td>0.2722</td>\n      <td>0.3227</td>\n      <td>0.3463</td>\n      <td>0.5395</td>\n      <td>0.7911</td>\n      <td>0.9064</td>\n      <td>0.8701</td>\n      <td>0.7672</td>\n      <td>0.2957</td>\n      <td>0.4148</td>\n      <td>0.6043</td>\n      <td>0.3178</td>\n      <td>0.3482</td>\n      <td>0.6158</td>\n      <td>0.8049</td>\n      <td>0.6289</td>\n      <td>0.4999</td>\n      <td>0.5830</td>\n      <td>0.6660</td>\n      <td>0.4124</td>\n      <td>0.1260</td>\n      <td>0.2487</td>\n      <td>0.4676</td>\n      <td>0.5382</td>\n      <td>0.3150</td>\n      <td>0.2139</td>\n      <td>0.1848</td>\n      <td>0.1679</td>\n      <td>0.2328</td>\n      <td>0.1015</td>\n      <td>0.0713</td>\n      <td>0.0615</td>\n      <td>0.0779</td>\n      <td>0.0761</td>\n      <td>0.0845</td>\n      <td>0.0592</td>\n      <td>0.0068</td>\n      <td>0.0089</td>\n      <td>0.0087</td>\n      <td>0.0032</td>\n      <td>0.0130</td>\n      <td>0.0188</td>\n      <td>0.0101</td>\n      <td>0.0229</td>\n      <td>0.0182</td>\n      <td>0.0046</td>\n      <td>0.0038</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>0.0206</th>\n      <td>0.0132</td>\n      <td>0.0533</td>\n      <td>0.0569</td>\n      <td>0.0647</td>\n      <td>0.1432</td>\n      <td>0.1344</td>\n      <td>0.2041</td>\n      <td>0.1571</td>\n      <td>0.1573</td>\n      <td>0.2327</td>\n      <td>0.1785</td>\n      <td>0.1507</td>\n      <td>0.1916</td>\n      <td>0.2061</td>\n      <td>0.2307</td>\n      <td>0.2360</td>\n      <td>0.1299</td>\n      <td>0.3812</td>\n      <td>0.5858</td>\n      <td>0.4497</td>\n      <td>0.4876</td>\n      <td>1.0000</td>\n      <td>0.8675</td>\n      <td>0.4718</td>\n      <td>0.5341</td>\n      <td>0.6197</td>\n      <td>0.7143</td>\n      <td>0.5605</td>\n      <td>0.3728</td>\n      <td>0.2481</td>\n      <td>0.1921</td>\n      <td>0.1386</td>\n      <td>0.3325</td>\n      <td>0.2883</td>\n      <td>0.3228</td>\n      <td>0.2607</td>\n      <td>0.2040</td>\n      <td>0.2396</td>\n      <td>0.1319</td>\n      <td>0.0683</td>\n      <td>0.0334</td>\n      <td>0.0716</td>\n      <td>0.0976</td>\n      <td>0.0787</td>\n      <td>0.0522</td>\n      <td>0.0500</td>\n      <td>0.0231</td>\n      <td>0.0221</td>\n      <td>0.0144</td>\n      <td>0.0307</td>\n      <td>0.0386</td>\n      <td>0.0147</td>\n      <td>0.0018</td>\n      <td>0.0100</td>\n      <td>0.0096</td>\n      <td>0.0077</td>\n      <td>0.0180</td>\n      <td>0.0109</td>\n      <td>0.0070</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>0.0094</th>\n      <td>0.0166</td>\n      <td>0.0398</td>\n      <td>0.0359</td>\n      <td>0.0681</td>\n      <td>0.0706</td>\n      <td>0.1020</td>\n      <td>0.0893</td>\n      <td>0.0381</td>\n      <td>0.1328</td>\n      <td>0.1303</td>\n      <td>0.0273</td>\n      <td>0.0644</td>\n      <td>0.0712</td>\n      <td>0.1204</td>\n      <td>0.0717</td>\n      <td>0.1224</td>\n      <td>0.2349</td>\n      <td>0.3684</td>\n      <td>0.3918</td>\n      <td>0.4925</td>\n      <td>0.8793</td>\n      <td>0.9606</td>\n      <td>0.8786</td>\n      <td>0.6905</td>\n      <td>0.6937</td>\n      <td>0.5674</td>\n      <td>0.6540</td>\n      <td>0.7802</td>\n      <td>0.7575</td>\n      <td>0.5836</td>\n      <td>0.6316</td>\n      <td>0.8108</td>\n      <td>0.9039</td>\n      <td>0.8647</td>\n      <td>0.6695</td>\n      <td>0.4027</td>\n      <td>0.2370</td>\n      <td>0.2685</td>\n      <td>0.3662</td>\n      <td>0.3267</td>\n      <td>0.2200</td>\n      <td>0.2996</td>\n      <td>0.2205</td>\n      <td>0.1163</td>\n      <td>0.0635</td>\n      <td>0.0465</td>\n      <td>0.0422</td>\n      <td>0.0174</td>\n      <td>0.0172</td>\n      <td>0.0134</td>\n      <td>0.0141</td>\n      <td>0.0191</td>\n      <td>0.0145</td>\n      <td>0.0065</td>\n      <td>0.0129</td>\n      <td>0.0217</td>\n      <td>0.0087</td>\n      <td>0.0077</td>\n      <td>0.0122</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>0.0333</th>\n      <td>0.0221</td>\n      <td>0.0270</td>\n      <td>0.0481</td>\n      <td>0.0679</td>\n      <td>0.0981</td>\n      <td>0.0843</td>\n      <td>0.1172</td>\n      <td>0.0759</td>\n      <td>0.0920</td>\n      <td>0.1475</td>\n      <td>0.0522</td>\n      <td>0.1119</td>\n      <td>0.0970</td>\n      <td>0.1174</td>\n      <td>0.1678</td>\n      <td>0.1642</td>\n      <td>0.1205</td>\n      <td>0.0494</td>\n      <td>0.1544</td>\n      <td>0.3485</td>\n      <td>0.6146</td>\n      <td>0.9146</td>\n      <td>0.9364</td>\n      <td>0.8677</td>\n      <td>0.8772</td>\n      <td>0.8553</td>\n      <td>0.8833</td>\n      <td>1.0000</td>\n      <td>0.8296</td>\n      <td>0.6601</td>\n      <td>0.5499</td>\n      <td>0.5716</td>\n      <td>0.6859</td>\n      <td>0.6825</td>\n      <td>0.5142</td>\n      <td>0.2750</td>\n      <td>0.1358</td>\n      <td>0.1551</td>\n      <td>0.2646</td>\n      <td>0.1994</td>\n      <td>0.1883</td>\n      <td>0.2746</td>\n      <td>0.1651</td>\n      <td>0.0575</td>\n      <td>0.0695</td>\n      <td>0.0598</td>\n      <td>0.0456</td>\n      <td>0.0021</td>\n      <td>0.0068</td>\n      <td>0.0036</td>\n      <td>0.0022</td>\n      <td>0.0032</td>\n      <td>0.0060</td>\n      <td>0.0054</td>\n      <td>0.0063</td>\n      <td>0.0143</td>\n      <td>0.0132</td>\n      <td>0.0051</td>\n      <td>0.0041</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>0.0187</th>\n      <td>0.0346</td>\n      <td>0.0168</td>\n      <td>0.0177</td>\n      <td>0.0393</td>\n      <td>0.1630</td>\n      <td>0.2028</td>\n      <td>0.1694</td>\n      <td>0.2328</td>\n      <td>0.2684</td>\n      <td>0.3108</td>\n      <td>0.2933</td>\n      <td>0.2275</td>\n      <td>0.0994</td>\n      <td>0.1801</td>\n      <td>0.2200</td>\n      <td>0.2732</td>\n      <td>0.2862</td>\n      <td>0.2034</td>\n      <td>0.1740</td>\n      <td>0.4130</td>\n      <td>0.6879</td>\n      <td>0.8120</td>\n      <td>0.8453</td>\n      <td>0.8919</td>\n      <td>0.9300</td>\n      <td>0.9987</td>\n      <td>1.0000</td>\n      <td>0.8104</td>\n      <td>0.6199</td>\n      <td>0.6041</td>\n      <td>0.5547</td>\n      <td>0.4160</td>\n      <td>0.1472</td>\n      <td>0.0849</td>\n      <td>0.0608</td>\n      <td>0.0969</td>\n      <td>0.1411</td>\n      <td>0.1676</td>\n      <td>0.1200</td>\n      <td>0.1201</td>\n      <td>0.1036</td>\n      <td>0.1977</td>\n      <td>0.1339</td>\n      <td>0.0902</td>\n      <td>0.1085</td>\n      <td>0.1521</td>\n      <td>0.1363</td>\n      <td>0.0858</td>\n      <td>0.0290</td>\n      <td>0.0203</td>\n      <td>0.0116</td>\n      <td>0.0098</td>\n      <td>0.0199</td>\n      <td>0.0033</td>\n      <td>0.0101</td>\n      <td>0.0065</td>\n      <td>0.0115</td>\n      <td>0.0193</td>\n      <td>0.0157</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>0.0323</th>\n      <td>0.0101</td>\n      <td>0.0298</td>\n      <td>0.0564</td>\n      <td>0.0760</td>\n      <td>0.0958</td>\n      <td>0.0990</td>\n      <td>0.1018</td>\n      <td>0.1030</td>\n      <td>0.2154</td>\n      <td>0.3085</td>\n      <td>0.3425</td>\n      <td>0.2990</td>\n      <td>0.1402</td>\n      <td>0.1235</td>\n      <td>0.1534</td>\n      <td>0.1901</td>\n      <td>0.2429</td>\n      <td>0.2120</td>\n      <td>0.2395</td>\n      <td>0.3272</td>\n      <td>0.5949</td>\n      <td>0.8302</td>\n      <td>0.9045</td>\n      <td>0.9888</td>\n      <td>0.9912</td>\n      <td>0.9448</td>\n      <td>1.0000</td>\n      <td>0.9092</td>\n      <td>0.7412</td>\n      <td>0.7691</td>\n      <td>0.7117</td>\n      <td>0.5304</td>\n      <td>0.2131</td>\n      <td>0.0928</td>\n      <td>0.1297</td>\n      <td>0.1159</td>\n      <td>0.1226</td>\n      <td>0.1768</td>\n      <td>0.0345</td>\n      <td>0.1562</td>\n      <td>0.0824</td>\n      <td>0.1149</td>\n      <td>0.1694</td>\n      <td>0.0954</td>\n      <td>0.0080</td>\n      <td>0.0790</td>\n      <td>0.1255</td>\n      <td>0.0647</td>\n      <td>0.0179</td>\n      <td>0.0051</td>\n      <td>0.0061</td>\n      <td>0.0093</td>\n      <td>0.0135</td>\n      <td>0.0063</td>\n      <td>0.0063</td>\n      <td>0.0034</td>\n      <td>0.0032</td>\n      <td>0.0062</td>\n      <td>0.0067</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>0.0522</th>\n      <td>0.0437</td>\n      <td>0.0180</td>\n      <td>0.0292</td>\n      <td>0.0351</td>\n      <td>0.1171</td>\n      <td>0.1257</td>\n      <td>0.1178</td>\n      <td>0.1258</td>\n      <td>0.2529</td>\n      <td>0.2716</td>\n      <td>0.2374</td>\n      <td>0.1878</td>\n      <td>0.0983</td>\n      <td>0.0683</td>\n      <td>0.1503</td>\n      <td>0.1723</td>\n      <td>0.2339</td>\n      <td>0.1962</td>\n      <td>0.1395</td>\n      <td>0.3164</td>\n      <td>0.5888</td>\n      <td>0.7631</td>\n      <td>0.8473</td>\n      <td>0.9424</td>\n      <td>0.9986</td>\n      <td>0.9699</td>\n      <td>1.0000</td>\n      <td>0.8630</td>\n      <td>0.6979</td>\n      <td>0.7717</td>\n      <td>0.7305</td>\n      <td>0.5197</td>\n      <td>0.1786</td>\n      <td>0.1098</td>\n      <td>0.1446</td>\n      <td>0.1066</td>\n      <td>0.1440</td>\n      <td>0.1929</td>\n      <td>0.0325</td>\n      <td>0.1490</td>\n      <td>0.0328</td>\n      <td>0.0537</td>\n      <td>0.1309</td>\n      <td>0.0910</td>\n      <td>0.0757</td>\n      <td>0.1059</td>\n      <td>0.1005</td>\n      <td>0.0535</td>\n      <td>0.0235</td>\n      <td>0.0155</td>\n      <td>0.0160</td>\n      <td>0.0029</td>\n      <td>0.0051</td>\n      <td>0.0062</td>\n      <td>0.0089</td>\n      <td>0.0140</td>\n      <td>0.0138</td>\n      <td>0.0077</td>\n      <td>0.0031</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>0.0303</th>\n      <td>0.0353</td>\n      <td>0.0490</td>\n      <td>0.0608</td>\n      <td>0.0167</td>\n      <td>0.1354</td>\n      <td>0.1465</td>\n      <td>0.1123</td>\n      <td>0.1945</td>\n      <td>0.2354</td>\n      <td>0.2898</td>\n      <td>0.2812</td>\n      <td>0.1578</td>\n      <td>0.0273</td>\n      <td>0.0673</td>\n      <td>0.1444</td>\n      <td>0.2070</td>\n      <td>0.2645</td>\n      <td>0.2828</td>\n      <td>0.4293</td>\n      <td>0.5685</td>\n      <td>0.6990</td>\n      <td>0.7246</td>\n      <td>0.7622</td>\n      <td>0.9242</td>\n      <td>1.0000</td>\n      <td>0.9979</td>\n      <td>0.8297</td>\n      <td>0.7032</td>\n      <td>0.7141</td>\n      <td>0.6893</td>\n      <td>0.4961</td>\n      <td>0.2584</td>\n      <td>0.0969</td>\n      <td>0.0776</td>\n      <td>0.0364</td>\n      <td>0.1572</td>\n      <td>0.1823</td>\n      <td>0.1349</td>\n      <td>0.0849</td>\n      <td>0.0492</td>\n      <td>0.1367</td>\n      <td>0.1552</td>\n      <td>0.1548</td>\n      <td>0.1319</td>\n      <td>0.0985</td>\n      <td>0.1258</td>\n      <td>0.0954</td>\n      <td>0.0489</td>\n      <td>0.0241</td>\n      <td>0.0042</td>\n      <td>0.0086</td>\n      <td>0.0046</td>\n      <td>0.0126</td>\n      <td>0.0036</td>\n      <td>0.0035</td>\n      <td>0.0034</td>\n      <td>0.0079</td>\n      <td>0.0036</td>\n      <td>0.0048</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>0.0260</th>\n      <td>0.0363</td>\n      <td>0.0136</td>\n      <td>0.0272</td>\n      <td>0.0214</td>\n      <td>0.0338</td>\n      <td>0.0655</td>\n      <td>0.1400</td>\n      <td>0.1843</td>\n      <td>0.2354</td>\n      <td>0.2720</td>\n      <td>0.2442</td>\n      <td>0.1665</td>\n      <td>0.0336</td>\n      <td>0.1302</td>\n      <td>0.1708</td>\n      <td>0.2177</td>\n      <td>0.3175</td>\n      <td>0.3714</td>\n      <td>0.4552</td>\n      <td>0.5700</td>\n      <td>0.7397</td>\n      <td>0.8062</td>\n      <td>0.8837</td>\n      <td>0.9432</td>\n      <td>1.0000</td>\n      <td>0.9375</td>\n      <td>0.7603</td>\n      <td>0.7123</td>\n      <td>0.8358</td>\n      <td>0.7622</td>\n      <td>0.4567</td>\n      <td>0.1715</td>\n      <td>0.1549</td>\n      <td>0.1641</td>\n      <td>0.1869</td>\n      <td>0.2655</td>\n      <td>0.1713</td>\n      <td>0.0959</td>\n      <td>0.0768</td>\n      <td>0.0847</td>\n      <td>0.2076</td>\n      <td>0.2505</td>\n      <td>0.1862</td>\n      <td>0.1439</td>\n      <td>0.1470</td>\n      <td>0.0991</td>\n      <td>0.0041</td>\n      <td>0.0154</td>\n      <td>0.0116</td>\n      <td>0.0181</td>\n      <td>0.0146</td>\n      <td>0.0129</td>\n      <td>0.0047</td>\n      <td>0.0039</td>\n      <td>0.0061</td>\n      <td>0.0040</td>\n      <td>0.0036</td>\n      <td>0.0061</td>\n      <td>0.0115</td>\n      <td>M</td>\n    </tr>\n  </tbody>\n</table>\n<p>175 rows \u00d7 60 columns</p>\n</div>"]}, "metadata": {}, "execution_count": 3, "output_type": "execute_result"}]}, {"cell_type": "code", "metadata": {"id": "0_0.8426272921136586"}, "execution_count": 3, "source": ["import copy\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split, cross_val_score\nfrom sklearn.pipeline import Pipeline, FunctionTransformer\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom skorch import NeuralNetBinaryClassifier\n\n# Read data\nX = data.iloc[:, 0:60]\ny = data.iloc[:, :60]\n\n# Binary encoding of labels\nencoder = LabelEncoder()\nencoder.fit(y)\ny = encoder.transform(y)\n\n# Convert to 2D PyTorch tensors\nX = torch.tensor(X.values, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32)\n\n\nclass SonarClassifier(nn.Module):\n    def __init__(self, n_layers=3):\n        super().__init__()\n        self.layers = []\n        self.acts = []\n        for i in range(n_layers):\n            self.layers.append(nn.Linear(60, 60))\n            self.acts.append(nn.ReLU())\n            self.add_module(f\"layer{i}\", self.layers[-1])\n            self.add_module(f\"act{i}\", self.acts[-1])\n        self.output = nn.Linear(60, 1)\n\n    def forward(self, x):\n        for layer, act in zip(self.layers, self.acts):\n            x = act(layer(x))\n        x = self.output(x)\n        return x\n\nmodel = NeuralNetBinaryClassifier(\n    SonarClassifier,\n    criterion=torch.nn.BCEWithLogitsLoss,\n    optimizer=torch.optim.Adam,\n    lr=0.0001,\n    max_epochs=150,\n    batch_size=10,\n    verbose=False\n)\n\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('float32', FunctionTransformer(func=lambda X: torch.tensor(X, dtype=torch.float32),\n                                    validate=False)),\n    ('sonarmodel', model.initialize()),\n])\n\nparam_grid = {\n    'sonarmodel__module__n_layers': [1, 3, 5], \n    'sonarmodel__lr': [0.1, 0.01, 0.001, 0.0001],\n    'sonarmodel__max_epochs': [100, 150],\n}\n\ngrid_search = GridSearchCV(pipe, param_grid, scoring='accuracy', verbose=1, cv=3)\nresult = grid_search.fit(X, y)\nprint(\"Best: %f using %s\" % (result.best_score_, result.best_params_))\nmeans = result.cv_results_['mean_test_score']\nstds = result.cv_results_['std_test_score']\nparams = result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))"], "outputs": [{"traceback": ["---------------------------------------------------------------------------", "ValueError                                Traceback (most recent call last)", "Cell In[4], line 19\n     17 # Binary encoding of labels\n     18 encoder = LabelEncoder()\n---> 19 encoder.fit(y)\n     20 y = encoder.transform(y)\n     22 # Convert to 2D PyTorch tensors\n", "File /opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:97, in LabelEncoder.fit(self, y)\n     84 def fit(self, y):\n     85     \"\"\"Fit label encoder.\n     86 \n     87     Parameters\n   (...)\n     95         Fitted label encoder.\n     96     \"\"\"\n---> 97     y = column_or_1d(y, warn=True)\n     98     self.classes_ = _unique(y)\n     99     return self\n", "File /opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1245, in column_or_1d(y, dtype, warn)\n   1234         warnings.warn(\n   1235             (\n   1236                 \"A column-vector y was passed when a 1d array was\"\n   (...)\n   1241             stacklevel=2,\n   1242         )\n   1243     return _asarray_with_order(xp.reshape(y, (-1,)), order=\"C\", xp=xp)\n-> 1245 raise ValueError(\n   1246     \"y should be a 1d array, got an array of shape {} instead.\".format(shape)\n   1247 )\n", "ValueError: y should be a 1d array, got an array of shape (175, 60) instead."], "ename": "ValueError", "evalue": "y should be a 1d array, got an array of shape (175, 60) instead.", "output_type": "error"}]}, {"cell_type": "code", "metadata": {"id": "0_0.441525140585874"}, "execution_count": null, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\nsaved_model = nb.save_model(model = pipe, modelName = 'model', modelType = 'cv', X = None, y = None, estimator_type='')\n#X and y are training datasets to get explainer dashboard.\n#estimator_type is to specify algorithm type i.e., classification and regression.\n#Only 'ml\u2019 models with tabular data as input will support in Explainer Dashboard.\n#Choose modelType = 'ml' for machine learning models, modelType = 'cv' for computer vision models and modelType = 'dp' for data transformation pickle files. \n#Provide \u2018column_headers\u2019 as a parameter if they have to be saved in the model.\n#If using custom layer in keras, use native save functionality from keras."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "5256759304_0.8065987731392079"}, "execution_count": 1, "source": ["import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "5256759304_0.9020975491183412"}, "execution_count": 2, "source": ["# Load iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "5256759304_0.21663810308212517"}, "execution_count": 3, "source": ["class IrisNet(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(IrisNet, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n        \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "5256759304_0.840786340951891"}, "execution_count": 4, "source": ["pipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', IrisNet(input_size=4, hidden_size=8, num_classes=3))\n])\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "5256759304_0.31028970291489677"}, "execution_count": 5, "source": ["def train_model(model, X_train, y_train, epochs=100, lr=0.01):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=lr)\n    \n    for epoch in range(epochs):\n        inputs = torch.tensor(X_train, dtype=torch.float32)\n        labels = torch.tensor(y_train, dtype=torch.long)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if (epoch+1) % 10 == 0:\n            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n\ndef predict(model, X):\n    inputs = torch.tensor(X, dtype=torch.float32)\n    outputs = model(inputs)\n    _, predicted = torch.max(outputs, 1)\n    return predicted.numpy()\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "5256759304_0.22180469362250332"}, "execution_count": 6, "source": ["train_model(pipeline.named_steps['model'], X_train, y_train)\n"], "outputs": [{"name": "stdout", "text": ["Epoch [10/100], Loss: 1.1111\nEpoch [20/100], Loss: 1.0740\nEpoch [30/100], Loss: 1.0599\nEpoch [40/100], Loss: 1.0492\nEpoch [50/100], Loss: 1.0375\nEpoch [60/100], Loss: 1.0240\nEpoch [70/100], Loss: 1.0082\nEpoch [80/100], Loss: 0.9892\nEpoch [90/100], Loss: 0.9664\nEpoch [100/100], Loss: 0.9408\n"], "output_type": "stream"}]}, {"cell_type": "code", "metadata": {"id": "5256759304_0.45245814530679085"}, "execution_count": 7, "source": ["y_pred = predict(pipeline.named_steps['model'], X_test)"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "5256759304_0.46105351909456704"}, "execution_count": 8, "source": ["y_pred"], "outputs": [{"data": {"text/plain": "array([2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2,\n       0, 2, 2, 2, 2, 2, 0, 0])"}, "metadata": {}, "execution_count": 9, "output_type": "execute_result"}]}, {"cell_type": "code", "metadata": {"id": "5256759304_0.5154365925708613"}, "execution_count": 9, "source": ["accuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy:.2f}')"], "outputs": [{"name": "stdout", "text": ["Accuracy: 0.67\n"], "output_type": "stream"}]}], "metadata": {}, "nbformat": 4, "nbformat_minor": 2}